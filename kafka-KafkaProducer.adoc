== [[KafkaProducer]] KafkaProducer -- Main Class For Kafka Producers

`KafkaProducer` is the main public class that a Kafka developer use to write <<kafka-producers.adoc#, Kafka producers>> that publish records to a Kafka cluster.

.KafkaProducer
image::images/KafkaProducer.png[align="center"]

`KafkaProducer` is a part of the public API and is <<creating-instance, created>> with properties and (key and value) serializers as configuration.

`KafkaProducer` follows the link:kafka-Producer.adoc[Producer contract].

`KafkaProducer` uses the <<sender, Kafka producer I/O thread>> that is responsible for sending produce requests to a Kafka cluster (on <<ioThread, kafka-producer-network-thread>> daemon thread of execution). The thread is started right when `KafkaProducer` is <<creating-instance, created>>.

[[metrics]]
`KafkaProducer` exposes performance metrics.

.KafkaProducer's Metrics
[cols="1m,1m,2",options="header",width="100%"]
|===
| Metric Name
| Recording Level
| Description

| buffer-exhausted-records
| INFO
| [[buffer-exhausted-records]] Recorded when `KafkaProducer` is requested to <<doSend, doSend>> and `BufferExhaustedException` is reported.

| errors
|
| [[errors]] Recorded when `KafkaProducer` is requested to <<doSend, doSend>> and there were exceptions reported (e.g. `ApiException`, `InterruptedException`, `BufferExhaustedException` and `KafkaException`)

| produce-throttle-time
|
| [[produce-throttle-time]] Recorded when `NetworkClient` is requested to <<kafka-NetworkClient.adoc#parseStructMaybeUpdateThrottleTimeMetrics, parseStructMaybeUpdateThrottleTimeMetrics>> (and a request was throttled due to quota violation)
|===

[[internal-registries]]
.KafkaProducer's Internal Properties (e.g. Registries and Counters)
[cols="1m,2",options="header",width="100%"]
|===
| Name
| Description

| maxBlockTimeMs
a| [[maxBlockTimeMs]] Time `KafkaProducer` uses to block when requesting <<partitionsFor, partitions for a topic>>.

NOTE: Use link:kafka-properties.adoc#max.block.ms[max.block.ms] Kafka property to set the value.

| metadata
a| [[metadata]] link:kafka-Metadata.adoc[Metadata]

* Created when `KafkaProducer` is <<creating-instance, created>> with the following properties:
** link:kafka-properties-retry-backoff-ms.adoc[retry.backoff.ms] for link:kafka-Metadata.adoc#refreshBackoffMs[refreshBackoffMs]
** link:kafka-properties.adoc#metadata.max.age.ms[metadata.max.age.ms] for link:kafka-Metadata.adoc#metadataExpireMs[metadataExpireMs]
** link:kafka-Metadata.adoc#allowAutoTopicCreation[allowAutoTopicCreation] flag enabled
** link:kafka-Metadata.adoc#topicExpiryEnabled[topicExpiryEnabled] flag enabled

* link:kafka-Metadata.adoc#update[Updated] with a bootstrap cluster when `KafkaProducer` is <<creating-instance, created>>

* Used in <<waitOnMetadata, waitOnMetadata>>

| sender
a| [[sender]] <<kafka-Sender.adoc#, Kafka producer I/O thread>> (aka `Sender`) that is <<kafka-Sender.adoc#run, started>> at the time `KafkaProducer` is <<creating-instance, created>>.

| ioThread
a| [[ioThread]] *kafka-producer-network-thread* daemon thread of execution for the <<sender, sender>>

| clientId
a| [[clientId]] Client ID per <<kafka-ProducerConfig.adoc#CLIENT_ID_CONFIG, CLIENT_ID_CONFIG>> (if defined) or `producer-[number]`

Used when:

* `KafkaProducer` is requested for <<logging, logging>>

* <<kafka-NetworkClient.adoc#, NetworkClient>> is created
|===

[[logging]]
[TIP]
====
Enable `DEBUG` or `TRACE` logging levels for `org.apache.kafka.clients.producer.KafkaProducer` logger to see what happens inside.

Add the following line to `config/tools-log4j.properties`:

```
log4j.logger.org.apache.kafka.clients.producer.KafkaProducer=TRACE
```

Refer to link:kafka-logging.adoc[Logging].
====

=== [[send]] Asynchronously Sending Record to Topic and Notifying Producer Interceptors -- `send` Method

[source, java]
----
Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback)
----

NOTE: `send` is a part of link:kafka-Producer.adoc#send[Producer Contract].

`send`...FIXME

=== [[doSend]] Asynchronously Sending Record to Topic -- `doSend` Internal Method

[source, java]
----
Future<RecordMetadata> doSend(ProducerRecord<K, V> record, Callback callback)
----

`doSend`...FIXME

NOTE: `doSend` is used exclusively when `KafkaProducer` <<send, asynchronously sends a record to a topic and notifies producer interceptors>>.

=== [[creating-instance]] Creating KafkaProducer Instance

`KafkaProducer` takes the following when created:

* [[config]] `ProducerConfig`
* [[keySerializer]] <<kafka-Serializer.adoc#, Serializer>> for keys
* [[valueSerializer]] <<kafka-Serializer.adoc#, Serializer>> for values
* [[metadata]] <<kafka-Metadata.adoc#, Metadata>>
* [[kafkaClient]] <<kafka-KafkaClient.adoc#, KafkaClient>>

`KafkaProducer` initializes the <<internal-registries, internal registries and counters>>.

=== [[configureClusterResourceListeners]] Configuring ClusterResourceListeners -- `configureClusterResourceListeners` Internal Method

[source, java]
----
ClusterResourceListeners configureClusterResourceListeners(
  Serializer<K> keySerializer,
  Serializer<V> valueSerializer,
  List<?>... candidateLists)
----

`configureClusterResourceListeners` creates a link:kafka-ClusterResourceListener.adoc#ClusterResourceListeners[ClusterResourceListeners] and registers `ClusterResourceListener` instances from the input `candidateLists`, `keySerializer` and `valueSerializer`.

[NOTE]
====
`configureClusterResourceListeners` is used exclusively when `KafkaProducer` is <<creating-instance, created>> (to create the <<metadata, Metadata>>) with the following input arguments:

* <<keySerializer, key>> and <<valueSerializer, value>> serializers (defined when `KafkaProducer` is created)

* link:kafka-ProducerInterceptor.adoc[ProducerInterceptors] from link:kafka-properties.adoc#interceptor.classes[interceptor.classes] Kafka property

* link:kafka-MetricsReporter.adoc[MetricsReporters] from link:kafka-properties.adoc#metric_reporters[metric.reporters] Kafka property
====

=== [[partitionsFor]] Requesting Partitions for Topic -- `partitionsFor` Method

[source, scala]
----
List<PartitionInfo> partitionsFor(String topic)
----

NOTE: `partitionsFor` is a part of link:kafka-Producer.adoc#partitionsFor[Producer Contract].

`partitionsFor` <<waitOnMetadata, waits on cluster metadata>> for the input `topic` and <<maxBlockTimeMs, max.block.ms>> time. Once retrieved, `partitionsFor` requests `Cluster` for the link:kafka-Cluster.adoc#partitionsForTopic[partitions].

=== [[waitOnMetadata]] Waiting for Cluster Metadata (with Partitions for Topic) -- `waitOnMetadata` Internal Recursive Method

[source, scala]
----
ClusterAndWaitTime waitOnMetadata(
  String topic,
  Integer partition,
  long maxWaitMs) throws InterruptedException
----

`waitOnMetadata` link:kafka-Metadata.adoc#add[adds] the input `topic` to <<metadata, Metadata>>.

`waitOnMetadata` first checks if the available cluster metadata could be current enough.

`waitOnMetadata` requests <<metadata, Metadata>> for the link:kafka-Metadata.adoc#fetch[current cluster information] and then requests the cluster for the link:kafka-Cluster.adoc#partitionCountForTopic[number of partitions] of the input `topic`.

If the cluster metadata is not current enough (i.e. the number of partitions is unavailable or the `partition` is above the current count), `waitOnMetadata` prints out the following TRACE message to the logs:

```
Requesting metadata update for topic [topic].
```

`waitOnMetadata` requests <<metadata, Metadata>> for link:kafka-Metadata.adoc#requestUpdate[update] and requests <<sender, Sender>> to link:kafka-Sender.adoc#wakeup[wake up].

`waitOnMetadata` then requests <<metadata, Metadata>> to link:kafka-Metadata.adoc#awaitUpdate[wait for a metadata update] and then <<metadata, Metadata>> for the link:kafka-Metadata.adoc#fetch[current cluster information].

`waitOnMetadata` keeps doing it until the link:kafka-Cluster.adoc#partitionCountForTopic[number of partitions] of the input `topic` is available.

`waitOnMetadata` reports a `TimeoutException` when `maxWaitMs` has elapsed.

```
Failed to update metadata after [maxWaitMs] ms.
```

`waitOnMetadata` reports a `TopicAuthorizationException` when the access to the `topic` is unauthorized.

`waitOnMetadata` reports a `KafkaException` when the `partition` is above the number of available partitions.

```
Invalid partition given with record: [partition] is not in the range [0...[partitionsCount]).
```

NOTE: `waitOnMetadata` is used when `KafkaProducer` <<partitionsFor, requests partitions for a topic>> and <<doSend, asynchronously sends a record to a topic>>.
