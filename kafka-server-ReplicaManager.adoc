== [[ReplicaManager]] ReplicaManager

`ReplicaManager` is <<creating-instance, created>> and immediately <<startup, started>> when `KafkaServer` is requested to <<kafka-server-KafkaServer.adoc#startup, start up>>.

.ReplicaManager and KafkaServer
image::images/ReplicaManager.png[align="center"]

When <<startup, started>>, `ReplicaManager` schedules <<kafka-server-scheduled-tasks.adoc#isr-expiration, isr-expiration>> and <<kafka-server-scheduled-tasks.adoc#isr-change-propagation, isr-change-propagation>> recurring tasks (every half of link:kafka-properties.adoc#replica.lag.time.max.ms[replica.lag.time.max.ms] property and 2500 ms, respectively).

`ReplicaManager` is a <<kafka-metrics-KafkaMetricsGroup.adoc#, KafkaMetricsGroup>>.

`ReplicaManager` uses the <<metadataCache, MetadataCache>> for the following:

* ...FIXME

[[internal-registries]]
.ReplicaManager's Internal Properties (e.g. Registries and Counters)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[replicaFetcherManager]] `replicaFetcherManager`
| link:kafka-ReplicaFetcherManager.adoc[ReplicaFetcherManager]

| [[allPartitions]] `allPartitions`
| Pool of `TopicPartition` and link:kafka-cluster-Partition.adoc[Partitions]

| [[isrChangeSet]] `isrChangeSet`
| Collection of `TopicPartition` that...FIXME

| [[lastIsrChangeMs]] `lastIsrChangeMs`
| Time when <<isrChangeSet, isrChangeSet>> has a new `TopicPartition` <<recordIsrChange, added>>.

| [[logDirFailureHandler]] `logDirFailureHandler`
| link:kafka-server-ReplicaManager-LogDirFailureHandler.adoc[LogDirFailureHandler]

| [[OfflinePartition]] `OfflinePartition`
|
|===

[[logging]]
[TIP]
====
Enable `TRACE` logging level for `kafka.server.ReplicaManager` logger to see what happens inside.

Add the following line to `log4j.properties`:

```
log4j.logger.kafka.server.ReplicaManager=TRACE
```

Refer to link:kafka-logging.adoc[Logging].
====

=== [[createReplicaFetcherManager]] `createReplicaFetcherManager` Internal Method

[source, scala]
----
createReplicaFetcherManager(
  metrics: Metrics
  time: Time
  threadNamePrefix: Option[String]
  quotaManager: ReplicationQuotaManager): ReplicaFetcherManager
----

`createReplicaFetcherManager`...FIXME

NOTE: `createReplicaFetcherManager` is used when...FIXME

=== [[shutdown]] `shutdown` Method

[source, scala]
----
shutdown(checkpointHW: Boolean = true): Unit
----

`shutdown`...FIXME

NOTE: `shutdown` is used when...FIXME

=== [[alterReplicaLogDirs]] `alterReplicaLogDirs` Method

[source, scala]
----
alterReplicaLogDirs(partitionDirs: Map[TopicPartition, String]): Map[TopicPartition, Errors]
----

`alterReplicaLogDirs`...FIXME

NOTE: `alterReplicaLogDirs` is used exclusively when `KafkaApis` link:kafka-server-KafkaApis.adoc#handleAlterReplicaLogDirsRequest[handles AlterReplicaLogDirs request].

=== [[becomeLeaderOrFollower]] `becomeLeaderOrFollower` Method

[source, scala]
----
becomeLeaderOrFollower(
  correlationId: Int,
  leaderAndIsrRequest: LeaderAndIsrRequest,
  onLeadershipChange: (Iterable[Partition], Iterable[Partition]) => Unit): LeaderAndIsrResponse
----

`becomeLeaderOrFollower`...FIXME

NOTE: `becomeLeaderOrFollower` is used exclusively when `KafkaApis` is requested to <<kafka-server-KafkaApis.adoc#handleLeaderAndIsrRequest, handle a LeaderAndIsr request>>.

=== [[makeFollowers]] `makeFollowers` Internal Method

[source, scala]
----
makeFollowers(
  controllerId: Int,
  epoch: Int,
  partitionState: Map[Partition, LeaderAndIsrRequest.PartitionState],
  correlationId: Int,
  responseMap: mutable.Map[TopicPartition, Errors]) : Set[Partition]
----

`makeFollowers`...FIXME

NOTE: `makeFollowers` is used exclusively when `ReplicaManager` is requested to <<becomeLeaderOrFollower, becomeLeaderOrFollower>>.

=== [[recordIsrChange]] `recordIsrChange` Method

[source, scala]
----
recordIsrChange(topicPartition: TopicPartition): Unit
----

`recordIsrChange` adds the input `topicPartition` to <<isrChangeSet, isrChangeSet>> internal registry and sets <<lastIsrChangeMs, lastIsrChangeMs>> to the current time.

NOTE: `recordIsrChange` is used exclusively when `Partition` does link:kafka-cluster-Partition.adoc#updateIsr[updateIsr]

=== [[updateFollowerLogReadResults]] `updateFollowerLogReadResults` Internal Method

[source, scala]
----
updateFollowerLogReadResults(
  replicaId: Int,
  readResults: Seq[(TopicPartition, LogReadResult)]): Seq[(TopicPartition, LogReadResult)]
----

`updateFollowerLogReadResults`...FIXME

NOTE: `updateFollowerLogReadResults` is used exclusively when `ReplicaManager` <<fetchMessages, fetches messages from the leader replica>>.

=== [[fetchMessages]] `fetchMessages` Method

[source, scala]
----
fetchMessages(
  timeout: Long,
  replicaId: Int,
  fetchMinBytes: Int,
  fetchMaxBytes: Int,
  hardMaxBytesLimit: Boolean,
  fetchInfos: Seq[(TopicPartition, FetchRequest.PartitionData)],
  quota: ReplicaQuota = UnboundedQuota,
  responseCallback: Seq[(TopicPartition, FetchPartitionData)] => Unit,
  isolationLevel: IsolationLevel): Unit
----

`fetchMessages`...FIXME

NOTE: `fetchMessages` is used exclusively when `KafkaApis` link:kafka-server-KafkaApis.adoc#handleFetchRequest[handles a Fetch request].

=== [[maybePropagateIsrChanges]] `maybePropagateIsrChanges` Method

[source, scala]
----
maybePropagateIsrChanges(): Unit
----

`maybePropagateIsrChanges`...FIXME

NOTE: `maybePropagateIsrChanges` is used exclusively when <<kafka-server-scheduled-tasks.adoc#isr-change-propagation, isr-change-propagation>> task is executed (every 2500 milliseconds).

=== [[creating-instance]] Creating ReplicaManager Instance

`ReplicaManager` takes the following when created:

* [[config]] <<kafka-KafkaConfig.adoc#, KafkaConfig>>
* [[metrics]] <<kafka-Metrics.adoc#, Metrics>>
* [[time]] `Time`
* [[zkClient]] <<kafka-zk-KafkaZkClient.adoc#, KafkaZkClient>>
* [[scheduler]] <<kafka-Scheduler.adoc#, Scheduler>>
* [[logManager]] <<kafka-LogManager.adoc#, LogManager>>
* [[isShuttingDown]] `isShuttingDown` flag
* [[quotaManagers]] `QuotaManagers`
* [[brokerTopicStats]] <<kafka-server-BrokerTopicStats.adoc#, BrokerTopicStats>>
* [[metadataCache]] <<kafka-server-MetadataCache.adoc#, MetadataCache>>
* [[logDirFailureChannel]] `LogDirFailureChannel`
* [[delayedProducePurgatory]] `DelayedOperationPurgatory[DelayedProduce]`
* [[delayedFetchPurgatory]] `DelayedOperationPurgatory[DelayedFetch]`
* [[delayedDeleteRecordsPurgatory]] `DelayedOperationPurgatory[DelayedDeleteRecords]`
* [[threadNamePrefix]] Optional thread name prefix

`ReplicaManager` initializes the <<internal-registries, internal registries and counters>>.

=== [[startup]] Starting ReplicaManager (and Scheduling ISR-Related Tasks) -- `startup` Method

[source, scala]
----
startup(): Unit
----

`startup` requests <<scheduler, Scheduler>> to link:kafka-KafkaScheduler.adoc#schedule[schedule the ISR-related tasks]:

. <<kafka-server-scheduled-tasks.adoc#isr-expiration, isr-expiration>>
. <<kafka-server-scheduled-tasks.adoc#isr-change-propagation, isr-change-propagation>>

`startup` then creates a <<logDirFailureHandler, LogDirFailureHandler>> and requests it to link:kafka-server-ReplicaManager-LogDirFailureHandler.adoc#start[start].

NOTE: `startup` uses `Scheduler` that was specified when `ReplicaManager` <<creating-instance, was created>>.

NOTE: `startup` is used exclusively when `KafkaServer` link:kafka-server-KafkaServer.adoc#startup[starts up].

=== [[maybeShrinkIsr]] `maybeShrinkIsr` Internal Method

[source, scala]
----
maybeShrinkIsr(): Unit
----

`maybeShrinkIsr` prints out the following TRACE message to the logs:

```
TRACE Evaluating ISR list of partitions to see which replicas can be removed from the ISR
```

`maybeShrinkIsr` requests the partitions (from <<allPartitions, allPartitions>> pool that are not <<OfflinePartition, offline partitions>>) to link:kafka-cluster-Partition.adoc#maybeShrinkIsr[maybeShrinkIsr] (with link:kafka-KafkaConfig.adoc#replicaLagTimeMaxMs[replicaLagTimeMaxMs] property).

NOTE: `maybeShrinkIsr` is used exclusively to schedule <<kafka-server-scheduled-tasks.adoc#isr-expiration, isr-expiration>> recurring task when `ReplicaManager` <<startup, starts up>>.

=== [[makeLeaders]] `makeLeaders` Internal Method

[source, scala]
----
makeLeaders(
  controllerId: Int,
  epoch: Int,
  partitionState: Map[Partition, LeaderAndIsrRequest.PartitionState],
  correlationId: Int,
  responseMap: mutable.Map[TopicPartition, Errors]): Set[Partition]
----

`makeLeaders`...FIXME

NOTE: `makeLeaders` is used exclusively when `ReplicaManager` is requested to <<becomeLeaderOrFollower, becomeLeaderOrFollower>>

=== [[describeLogDirs]] `describeLogDirs` Method

[source, scala]
----
describeLogDirs(partitions: Set[TopicPartition]): Map[String, LogDirInfo]
----

`describeLogDirs`...FIXME

NOTE: `describeLogDirs` is used exclusively when `KafkaApis` is requested to <<kafka-server-KafkaApis.adoc#handleDescribeLogDirsRequest, handle a DescribeLogDirs request>>.

=== [[getLog]] `getLog` Method

[source, scala]
----
getLog(topicPartition: TopicPartition): Option[Log]
----

`getLog`...FIXME

NOTE: `getLog` is used when...FIXME

=== [[startHighWaterMarksCheckPointThread]] `startHighWaterMarksCheckPointThread` Method

[source, scala]
----
startHighWaterMarksCheckPointThread(): Unit
----

`startHighWaterMarksCheckPointThread`...FIXME

NOTE: `startHighWaterMarksCheckPointThread` is used when...FIXME

=== [[checkpointHighWatermarks]] `checkpointHighWatermarks` Method

[source, scala]
----
checkpointHighWatermarks(): Unit
----

`checkpointHighWatermarks`...FIXME

NOTE: `checkpointHighWatermarks` is used when...FIXME

=== [[shutdownIdleReplicaAlterLogDirsThread]] `shutdownIdleReplicaAlterLogDirsThread` Method

[source, scala]
----
shutdownIdleReplicaAlterLogDirsThread(): Unit
----

`shutdownIdleReplicaAlterLogDirsThread`...FIXME

NOTE: `shutdownIdleReplicaAlterLogDirsThread` is used when...FIXME

=== [[handleLogDirFailure]] `handleLogDirFailure` Method

[source, scala]
----
handleLogDirFailure(dir: String, sendZkNotification: Boolean = true): Unit
----

`handleLogDirFailure`...FIXME

NOTE: `handleLogDirFailure` is used exclusively when `LogDirFailureHandler` is requested to <<kafka-server-ReplicaManager-LogDirFailureHandler.adoc#doWork, doWork>>.

=== [[maybeUpdateMetadataCache]] `maybeUpdateMetadataCache` Method

[source, scala]
----
maybeUpdateMetadataCache(
  correlationId: Int,
  updateMetadataRequest: UpdateMetadataRequest) : Seq[TopicPartition]
----

`maybeUpdateMetadataCache`...FIXME

NOTE: `maybeUpdateMetadataCache` is used exclusively when `KafkaApis` is requested to <<kafka-server-KafkaApis.adoc#handleUpdateMetadataRequest, handleUpdateMetadataRequest>>.

=== [[appendRecords]] Appending Records -- `appendRecords` Method

[source, scala]
----
appendRecords(
  timeout: Long,
  requiredAcks: Short,
  internalTopicsAllowed: Boolean,
  isFromClient: Boolean,
  entriesPerPartition: Map[TopicPartition, MemoryRecords],
  responseCallback: Map[TopicPartition, PartitionResponse] => Unit,
  delayedProduceLock: Option[Lock] = None,
  recordConversionStatsCallback: Map[TopicPartition, RecordConversionStats] => Unit = _ => ()): Unit
----

`appendRecords`...FIXME

[NOTE]
====
`appendRecords` is used when:

* `GroupMetadataManager` is requested to <<kafka-coordinator-group-GroupMetadataManager.adoc#appendForGroup, request the ReplicaManager to append records>>

* `TransactionStateManager` is requested to <<kafka-TransactionStateManager.adoc#enableTransactionalIdExpiration, enableTransactionalIdExpiration>> and <<kafka-TransactionStateManager.adoc#appendTransactionToLog, appendTransactionToLog>>

* `KafkaApis` is requested to <<kafka-server-KafkaApis.adoc#handleProduceRequest, handleProduceRequest>> and <<kafka-server-KafkaApis.adoc#handleWriteTxnMarkersRequest, handleWriteTxnMarkersRequest>>
====

=== [[appendToLocalLog]] `appendToLocalLog` Internal Method

[source, scala]
----
appendToLocalLog(
  internalTopicsAllowed: Boolean,
  isFromClient: Boolean,
  entriesPerPartition: Map[TopicPartition, MemoryRecords],
  requiredAcks: Short): Map[TopicPartition, LogAppendResult]
----

`appendToLocalLog` prints out the following TRACE message to the logs:

```
Append [[entriesPerPartition]] to local log
```

For every partition in the given `entriesPerPartition`, `appendToLocalLog` <<getPartitionOrException, gets the partition (or throws an exception)>> (with `expectLeader` flag enabled) and then request the `Partition` to <<kafka-cluster-Partition.adoc#appendRecordsToLeader, appendRecordsToLeader>>.

`appendToLocalLog`...FIXME

In the end, `appendToLocalLog` prints out the following TRACE message to the logs:

```
[sizeInBytes] written to log [topicPartition] beginning at offset [firstOffset] and ending at offset [lastOffset]
```

In case of exceptions, `appendToLocalLog`...FIXME

NOTE: `appendToLocalLog` is used exclusively when `ReplicaManager` is requested to <<appendRecords, append records>>.

=== [[getPartitionOrException]] Getting Partition Or Throwing Exception -- `getPartitionOrException` Method

[source, scala]
----
getPartitionOrException(
  topicPartition: TopicPartition,
  expectLeader: Boolean): Partition
----

`getPartitionOrException` <<getPartition, gets the partition>> if available or throws one of the following exceptions:

* `KafkaStorageException` when the partition is offline
+
```
Partition [topicPartition] is in an offline log directory
```

* `NotLeaderForPartitionException`
+
```
Broker [localBrokerId] is not a replica of [topicPartition]
```

* `ReplicaNotAvailableException`
+
```
Partition [topicPartition] is not available
```

* `UnknownTopicOrPartitionException`
+
```
Partition [topicPartition] doesn't exist
```

NOTE: `getPartitionOrException` is used when...FIXME

=== [[getPartition]] Getting Partition by TopicPartition -- `getPartition` Method

[source, scala]
----
getPartition(topicPartition: TopicPartition): Option[Partition]
----

`getPartition` gets the <<kafka-cluster-Partition.adoc#, partition>> for the given `TopicPartition`.

NOTE: `getPartition` is used when...FIXME
