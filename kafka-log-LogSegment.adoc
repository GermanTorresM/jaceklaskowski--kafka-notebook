== [[LogSegment]] LogSegment

`LogSegment` is a segment of a <<kafka-log-Log.adoc#, partition log>>.

`LogSegment` has three components: a <<log, log>> file with <<lazyOffsetIndex, index>> and <<lazyTimeIndex, time index>> files. With a given <<baseOffset, base offset>> the files would be: a `[base offset].index`, a `[base offset].log`, a `[base offset].timeindex` files, e.g.

```
$ tree /tmp/kafka-logs/t1-1/
/tmp/kafka-logs/t1-1/
├── 00000000000000000000.index
├── 00000000000000000000.log
├── 00000000000000000000.timeindex
└── leader-epoch-checkpoint
```

`LogSegment` is <<creating-instance, created>> (indirectly using <<open, LogSegment.open>> utility) when:

* `Log` is requested to <<kafka-log-Log.adoc#loadSegmentFiles, loadSegmentFiles>> (for every log file), <<kafka-log-Log.adoc#completeSwapOperations, completeSwapOperations>>, <<kafka-log-Log.adoc#loadSegments, loadSegments>>, <<kafka-log-Log.adoc#recoverLog, recoverLog>>, <<kafka-log-Log.adoc#roll, roll>>, and <<kafka-log-Log.adoc#truncateFullyAndStartAt, truncateFullyAndStartAt>>

* `LogCleaner` is requested to <<kafka-log-LogCleaner.adoc#createNewCleanedSegment, createNewCleanedSegment>>

.Example
[source, scala]
----
import java.nio.file.Path
val file = Path.of("/tmp/logsegment").toFile
val config = LogConfig()
import kafka.log.{LogConfig, LogSegment}
import org.apache.kafka.common.utils.Time
scala> val segment = LogSegment.open(file, baseOffset = 0, config, time = Time.SYSTEM)
java.nio.file.NoSuchFileException: /tmp/logsegment/00000000000000000000.log
  at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
  at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
  at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
  at java.base/sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:182)
  at java.base/java.nio.channels.FileChannel.open(FileChannel.java:292)
  at java.base/java.nio.channels.FileChannel.open(FileChannel.java:345)
  at org.apache.kafka.common.record.FileRecords.openChannel(FileRecords.java:448)
  at org.apache.kafka.common.record.FileRecords.open(FileRecords.java:411)
  at org.apache.kafka.common.record.FileRecords.open(FileRecords.java:420)
  at kafka.log.LogSegment$.open(LogSegment.scala:657)
  ... 36 elided
----

=== [[creating-instance]] Creating LogSegment Instance

`LogSegment` takes the following to be created:

* [[log]] <<kafka-common-record-FileRecords.adoc#, FileRecords>>
* [[lazyOffsetIndex]] Lazy <<kafka-log-OffsetIndex.adoc#, OffsetIndex>> with deferred loading (`LazyIndex[OffsetIndex]`)
* [[lazyTimeIndex]] Lazy <<kafka-log-TimeIndex.adoc#, TimeIndex>> with deferred loading (`LazyIndex[TimeIndex]`)
* [[txnIndex]] <<kafka-log-TransactionIndex.adoc#, TransactionIndex>>
* [[baseOffset]] Base offset
* [[indexIntervalBytes]] `indexIntervalBytes`
* [[rollJitterMs]] `rollJitterMs`
* [[time]] `Time`

`LogSegment` initializes the <<internal-properties, internal properties>>.

=== [[open]] Opening Log Segment -- `open` Utility

[source, scala]
----
open(
  dir: File,
  baseOffset: Long,
  config: LogConfig,
  time: Time,
  fileAlreadyExists: Boolean = false,
  initFileSize: Int = 0,
  preallocate: Boolean = false,
  fileSuffix: String = ""): LogSegment
----

`open` uses the following configuration properties (of the given <<kafka-log-LogConfig.adoc#, LogConfig>>):

* <<kafka-log-LogConfig.adoc#maxIndexSize, segment.index.bytes>> for the `maxIndexSize`

* <<kafka-log-LogConfig.adoc#indexInterval, index.interval.bytes>> for the `indexIntervalBytes`

* <<kafka-log-LogConfig.adoc#randomSegmentJitter, segment.index.bytes>> for the `rollJitterMs`

`open` creates a new <<creating-instance, LogSegment>> with the following files in the given `dir` directory:

* <<kafka-log-Log.adoc#logFile, Creates a log file>> (with the given `baseOffset`, and `fileSuffix`) and opens it (using `FileRecords.open`)

* <<kafka-log-Log.adoc#offsetIndexFile, Creates an offset index file>>

* <<kafka-log-Log.adoc#timeIndexFile, Creates a time index file>>

* <<kafka-log-Log.adoc#transactionIndexFile, Creates a transaction index file>>

[NOTE]
====
`open` is used when:

* `Log` is requested to <<kafka-log-Log.adoc#loadSegmentFiles, loadSegmentFiles>> (for every log file), <<kafka-log-Log.adoc#completeSwapOperations, completeSwapOperations>>, <<kafka-log-Log.adoc#loadSegments, loadSegments>>, <<kafka-log-Log.adoc#recoverLog, recoverLog>>, <<kafka-log-Log.adoc#roll, roll>>, and <<kafka-log-Log.adoc#truncateFullyAndStartAt, truncateFullyAndStartAt>>

* `LogCleaner` is requested to <<kafka-log-LogCleaner.adoc#createNewCleanedSegment, createNewCleanedSegment>>
====

=== [[offsetIndex]] LogSegment and OffsetIndex

[source, scala]
----
offsetIndex: OffsetIndex
----

When <<creating-instance, created>>, `LogSegment` is given an <<lazyOffsetIndex, OffsetIndex>> with deferred loading (`LazyIndex[OffsetIndex]`).

`offsetIndex` simply gets (_unwraps_) the <<kafka-log-OffsetIndex.adoc#, OffsetIndex>>.

`offsetIndex` is used for the following...FIXME

=== [[recover]] `recover` Method

[source, scala]
----
recover(
  producerStateManager: ProducerStateManager,
  leaderEpochCache: Option[LeaderEpochFileCache] = None): Int
----

`recover`...FIXME

NOTE: `recover` is used exclusively when `Log` is requested to <<kafka-log-Log.adoc#recoverSegment, recover a log segment>>.

==== [[updateProducerState]] `updateProducerState` Internal Method

[source, scala]
----
updateProducerState(
  producerStateManager: ProducerStateManager,
  batch: RecordBatch): Unit
----

`updateProducerState`...FIXME

NOTE: `updateProducerState` is used when `LogSegment` is requested to <<recover, recover>>.

=== [[sanityCheck]] `sanityCheck` Method

[source, scala]
----
sanityCheck(
  timeIndexFileNewlyCreated: Boolean): Unit
----

`sanityCheck`...FIXME

NOTE: `sanityCheck` is used exclusively when `Log` is requested to <<kafka-log-Log.adoc#loadSegments, loadSegments>> (when <<kafka-log-Log.adoc#creating-instance-loadSegments, created>>).

=== [[updateDir]] `updateDir` Method

[source, scala]
----
updateDir(
  dir: File): Unit
----

`updateDir`...FIXME

NOTE: `updateDir` is used exclusively when `Log` is requested to <<kafka-log-Log.adoc#renameDir, renameDir>>.

=== [[changeFileSuffixes]] `changeFileSuffixes` Method

[source, scala]
----
changeFileSuffixes(
  oldSuffix: String,
  newSuffix: String): Unit
----

`changeFileSuffixes`...FIXME

NOTE: `changeFileSuffixes` is used when `Log` is requested to <<kafka-log-Log.adoc#asyncDeleteSegment, asyncDeleteSegment>> and <<kafka-log-Log.adoc#replaceSegments, replaceSegments>>.

=== [[flush]] `flush` Method

[source, scala]
----
flush(): Unit
----

`flush`...FIXME

[NOTE]
====
`flush` is used when:

* `Log` is requested to <<kafka-log-Log.adoc#flush, flush>> and <<kafka-log-Log.adoc#splitOverflowedSegment, splitOverflowedSegment>>

* `Cleaner` is requested to <<kafka-log-Cleaner.adoc#clean, clean a log>> (and <<kafka-log-Cleaner.adoc#cleanSegments, cleanSegments>>)
====

=== [[deleteIfExists]] `deleteIfExists` Method

[source, scala]
----
deleteIfExists(): Unit
----

`deleteIfExists`...FIXME

NOTE: `deleteIfExists` is used when...FIXME

=== [[deleteIfExists-utility]] `deleteIfExists` Utility

[source, scala]
----
deleteIfExists(
  dir: File,
  baseOffset: Long,
  fileSuffix: String = ""): Unit
----

`deleteIfExists`...FIXME

NOTE: `deleteIfExists` is used when...FIXME

=== [[resizeIndexes]] `resizeIndexes` Method

[source, scala]
----
resizeIndexes(
  size: Int): Unit
----

`resizeIndexes`...FIXME

NOTE: `resizeIndexes` is used when...FIXME

=== [[largestTimestamp]] `largestTimestamp` Method

[source, scala]
----
largestTimestamp: Long
----

`largestTimestamp`...FIXME

NOTE: `largestTimestamp` is used when...FIXME

=== [[shouldRoll]] `shouldRoll` Method

[source, scala]
----
shouldRoll(
  rollParams: RollParams): Boolean
----

`shouldRoll`...FIXME

NOTE: `shouldRoll` is used exclusively when `Log` is requested to <<kafka-log-Log.adoc#maybeRoll, maybeRoll>> (while <<kafka-log-Log.adoc#append, appending records>>).

=== [[timeWaitedForRoll]] `timeWaitedForRoll` Method

[source, scala]
----
timeWaitedForRoll(
  now: Long,
  messageTimestamp: Long) : Long
----

`timeWaitedForRoll`...FIXME

NOTE: `timeWaitedForRoll` is used exclusively when `LogSegment` is requested to <<shouldRoll, shouldRoll>>.

=== [[append]] `append` Method

[source, scala]
----
append(
  largestOffset: Long,
  largestTimestamp: Long,
  shallowOffsetOfMaxTimestamp: Long,
  records: MemoryRecords): Unit
----

`append`...FIXME

[NOTE]
====
`append` is used exclusively when:

* `Log` is requested to <<kafka-log-Log.adoc#append, append records>>

* `Cleaner` is requested to <<kafka-log-Cleaner.adoc#clean, clean a log>> (and <<kafka-log-Cleaner.adoc#cleanInto, cleanInto>>)

* `LogSegment` is requested to <<appendChunkFromFile, appendChunkFromFile>>
====

=== [[appendFromFile]] `appendFromFile` Method

[source, scala]
----
appendFromFile(
  records: FileRecords,
  start: Int): Int
----

`appendFromFile`...FIXME

NOTE: `appendFromFile` is used exclusively when `Log` is requested to <<kafka-log-Log.adoc#splitOverflowedSegment, splitOverflowedSegment>>.

==== [[appendChunkFromFile]] `appendChunkFromFile` Internal Method

[source, scala]
----
appendChunkFromFile(
  records: FileRecords,
  position: Int,
  bufferSupplier: BufferSupplier): Int
----

`appendChunkFromFile`...FIXME

NOTE: `appendChunkFromFile` is used when `LogSegment` is requested to <<appendFromFile, appendFromFile>>.

=== [[truncateTo]] Truncating To Offset -- `truncateTo` Method

[source, scala]
----
truncateTo(
  offset: Long): Int
----

`truncateTo`...FIXME

NOTE: `truncateTo` is used when `Log` is <<kafka-log-Log.adoc#creating-instance-loadSegments, created>> (and in turn <<kafka-log-Log.adoc#recoverLog, recoverLog>>) and <<kafka-log-Log.adoc#truncateTo, truncateTo>>.

=== [[updateTxnIndex]] `updateTxnIndex` Method

[source, scala]
----
updateTxnIndex(
  completedTxn: CompletedTxn,
  lastStableOffset: Long): Unit
----

`updateTxnIndex`...FIXME

[NOTE]
====
`updateTxnIndex` is used when:

* `Log` is requested to <<kafka-log-Log.adoc#append, append records>>

* `LogSegment` is requested to <<updateProducerState, updateProducerState>>
====

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| created
a| [[created]] Time(stamp) when this `LogSegment` was <<creating-instance, created>> or <<truncateTo, truncated completely>> (to `0`)

Used exclusively when `LogSegment` is requested for the <<timeWaitedForRoll, time it has waited to be rolled>>

| bytesSinceLastIndexEntry
a| [[bytesSinceLastIndexEntry]]

Used when...FIXME

| rollingBasedTimestamp
a| [[rollingBasedTimestamp]]

Used when...FIXME

| _maxTimestampSoFar
a| [[_maxTimestampSoFar]]

Used when...FIXME

| _offsetOfMaxTimestampSoFar
a| [[_offsetOfMaxTimestampSoFar]]

Used when...FIXME

|===
