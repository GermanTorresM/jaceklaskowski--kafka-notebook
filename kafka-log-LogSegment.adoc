== [[LogSegment]] LogSegment

`LogSegment` is a segment of a <<kafka-log-Log.adoc#, partition log>>.

`LogSegment` has three components: a <<log, log>> file with <<lazyOffsetIndex, index>> and <<lazyTimeIndex, time index>> files. With a given <<baseOffset, base offset>> the files would be: a `[base offset].index`, a `[base offset].log`, a `[base offset].timeindex` files, e.g.

```
$ tree /tmp/kafka-logs/t1-1/
/tmp/kafka-logs/t1-1/
├── 00000000000000000000.index
├── 00000000000000000000.log
├── 00000000000000000000.timeindex
└── leader-epoch-checkpoint
```

`LogSegment` is <<creating-instance, created>> (indirectly using <<open, open>>) when:

* `Log` is requested to <<kafka-log-Log.adoc#loadSegmentFiles, loadSegmentFiles>> (for every log file), <<kafka-log-Log.adoc#completeSwapOperations, completeSwapOperations>>, <<kafka-log-Log.adoc#loadSegments, loadSegments>>, <<kafka-log-Log.adoc#recoverLog, recoverLog>>, <<kafka-log-Log.adoc#roll, roll>>, and <<kafka-log-Log.adoc#truncateFullyAndStartAt, truncateFullyAndStartAt>>

* `LogCleaner` is requested to <<kafka-log-LogCleaner.adoc#createNewCleanedSegment, createNewCleanedSegment>>

=== [[creating-instance]] Creating LogSegment Instance

`LogSegment` takes the following to be created:

* [[log]] `FileRecords`
* [[lazyOffsetIndex]] Lazy <<kafka-log-OffsetIndex.adoc#, OffsetIndex>> with deferred loading (`LazyIndex[OffsetIndex]`)
* [[lazyTimeIndex]] `LazyIndex[TimeIndex]`
* [[txnIndex]] `TransactionIndex`
* [[baseOffset]] Base offset
* [[indexIntervalBytes]] `indexIntervalBytes`
* [[rollJitterMs]] `rollJitterMs`
* [[time]] `Time`

`LogSegment` initializes the <<internal-properties, internal properties>>.

=== [[offsetIndex]] LogSegment and OffsetIndex

[source, scala]
----
offsetIndex: OffsetIndex
----

When <<creating-instance, created>>, `LogSegment` is given an <<lazyOffsetIndex, OffsetIndex>> with deferred loading (`LazyIndex[OffsetIndex]`).

`offsetIndex` simply gets (_unwraps_) the <<kafka-log-OffsetIndex.adoc#, OffsetIndex>>.

`offsetIndex` is used for the following...FIXME

=== [[recover]] `recover` Method

[source, scala]
----
recover(
  producerStateManager: ProducerStateManager,
  leaderEpochCache: Option[LeaderEpochFileCache] = None): Int
----

`recover`...FIXME

NOTE: `recover` is used exclusively when `Log` is requested to <<kafka-log-Log.adoc#recoverSegment, recover a log segment>>.

=== [[sanityCheck]] `sanityCheck` Method

[source, scala]
----
sanityCheck(
  timeIndexFileNewlyCreated: Boolean): Unit
----

`sanityCheck`...FIXME

NOTE: `sanityCheck` is used exclusively when `Log` is requested to <<kafka-log-Log.adoc#loadSegments, loadSegments>> (when <<kafka-log-Log.adoc#creating-instance-loadSegments, created>>).

=== [[updateDir]] `updateDir` Method

[source, scala]
----
updateDir(
  dir: File): Unit
----

`updateDir`...FIXME

NOTE: `updateDir` is used exclusively when `Log` is requested to <<kafka-log-Log.adoc#renameDir, renameDir>>.

=== [[changeFileSuffixes]] `changeFileSuffixes` Method

[source, scala]
----
changeFileSuffixes(
  oldSuffix: String,
  newSuffix: String): Unit
----

`changeFileSuffixes`...FIXME

NOTE: `changeFileSuffixes` is used when `Log` is requested to <<kafka-log-Log.adoc#asyncDeleteSegment, asyncDeleteSegment>> and <<kafka-log-Log.adoc#replaceSegments, replaceSegments>>.

=== [[flush]] `flush` Method

[source, scala]
----
flush(): Unit
----

`flush`...FIXME

[NOTE]
====
`flush` is used when:

* `Log` is requested to <<kafka-log-Log.adoc#flush, flush>> and <<kafka-log-Log.adoc#splitOverflowedSegment, splitOverflowedSegment>>

* `Cleaner` is requested to <<kafka-log-Cleaner.adoc#clean, clean a log>> (and <<kafka-log-Cleaner.adoc#cleanSegments, cleanSegments>>)
====

=== [[open]] Creating LogSegment Instance -- `open` Utility

[source, scala]
----
open(
  dir: File,
  baseOffset: Long,
  config: LogConfig,
  time: Time,
  fileAlreadyExists: Boolean = false,
  initFileSize: Int = 0,
  preallocate: Boolean = false,
  fileSuffix: String = ""): LogSegment
----

`open` uses the following configuration properties (of the given <<kafka-log-LogConfig.adoc#, LogConfig>>):

* <<kafka-log-LogConfig.adoc#maxIndexSize, segment.index.bytes>> for the `maxIndexSize`

* <<kafka-log-LogConfig.adoc#indexInterval, index.interval.bytes>> for the `indexIntervalBytes`

* <<kafka-log-LogConfig.adoc#randomSegmentJitter, segment.index.bytes>> for the `rollJitterMs`

`open` creates a new <<creating-instance, LogSegment>> with the following files in the given `dir` directory:

* <<kafka-log-Log.adoc#logFile, Creates a log file>> (with the given `baseOffset`, and `fileSuffix`) and opens it (using `FileRecords.open`)

* <<kafka-log-Log.adoc#offsetIndexFile, Creates an offset index file>>

* <<kafka-log-Log.adoc#timeIndexFile, Creates a time index file>>

* <<kafka-log-Log.adoc#transactionIndexFile, Creates a transaction index file>>

[NOTE]
====
`open` is used when:

* `Log` is requested to <<kafka-log-Log.adoc#loadSegmentFiles, loadSegmentFiles>> (for every log file), <<kafka-log-Log.adoc#completeSwapOperations, completeSwapOperations>>, <<kafka-log-Log.adoc#loadSegments, loadSegments>>, <<kafka-log-Log.adoc#recoverLog, recoverLog>>, <<kafka-log-Log.adoc#roll, roll>>, and <<kafka-log-Log.adoc#truncateFullyAndStartAt, truncateFullyAndStartAt>>

* `LogCleaner` is requested to <<kafka-log-LogCleaner.adoc#createNewCleanedSegment, createNewCleanedSegment>>
====

=== [[deleteIfExists]] `deleteIfExists` Utility

[source, scala]
----
deleteIfExists(
  dir: File,
  baseOffset: Long,
  fileSuffix: String = ""): Unit
----

`deleteIfExists`...FIXME

NOTE: `deleteIfExists` is used when...FIXME

=== [[resizeIndexes]] `resizeIndexes` Method

[source, scala]
----
resizeIndexes(size: Int): Unit
----

`resizeIndexes`...FIXME

NOTE: `resizeIndexes` is used when...FIXME

=== [[largestTimestamp]] `largestTimestamp` Method

[source, scala]
----
largestTimestamp: Long
----

`largestTimestamp`...FIXME

NOTE: `largestTimestamp` is used when...FIXME

=== [[shouldRoll]] `shouldRoll` Method

[source, scala]
----
shouldRoll(
  rollParams: RollParams): Boolean
----

`shouldRoll`...FIXME

NOTE: `shouldRoll` is used exclusively when `Log` is requested to <<kafka-log-Log.adoc#maybeRoll, maybeRoll>> (while <<kafka-log-Log.adoc#append, appending records>>).

=== [[timeWaitedForRoll]] `timeWaitedForRoll` Method

[source, scala]
----
timeWaitedForRoll(
  now: Long,
  messageTimestamp: Long) : Long
----

`timeWaitedForRoll`...FIXME

NOTE: `timeWaitedForRoll` is used exclusively when `LogSegment` is requested to <<shouldRoll, shouldRoll>>.

=== [[append]] `append` Method

[source, scala]
----
append(
  largestOffset: Long,
  largestTimestamp: Long,
  shallowOffsetOfMaxTimestamp: Long,
  records: MemoryRecords): Unit
----

`append`...FIXME

[NOTE]
====
`append` is used exclusively when:

* `Log` is requested to <<kafka-log-Log.adoc#append, append records>>

* `Cleaner` is requested to <<kafka-log-Cleaner.adoc#clean, clean a log>> (and <<kafka-log-Cleaner.adoc#cleanInto, cleanInto>>)

* `LogSegment` is requested to <<appendChunkFromFile, appendChunkFromFile>>
====

=== [[appendFromFile]] `appendFromFile` Method

[source, scala]
----
appendFromFile(
  records: FileRecords,
  start: Int): Int
----

`appendFromFile`...FIXME

NOTE: `appendFromFile` is used exclusively when `Log` is requested to <<kafka-log-Log.adoc#splitOverflowedSegment, splitOverflowedSegment>>.

=== [[appendChunkFromFile]] `appendChunkFromFile` Internal Method

[source, scala]
----
appendChunkFromFile(
  records: FileRecords,
  position: Int,
  bufferSupplier: BufferSupplier): Int
----

`appendChunkFromFile`...FIXME

NOTE: `appendChunkFromFile` is used exclusively when `LogSegment` is requested to <<appendFromFile, appendFromFile>>.

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| created
a| [[created]] Time(stamp) when this `LogSegment` was <<creating-instance, created>> or <<truncateTo, truncated completely>> (to `0`)

Used exclusively when `LogSegment` is requested for the <<timeWaitedForRoll, time it has waited to be rolled>>

| bytesSinceLastIndexEntry
a| [[bytesSinceLastIndexEntry]]

Used when...FIXME

| rollingBasedTimestamp
a| [[rollingBasedTimestamp]]

Used when...FIXME

| _maxTimestampSoFar
a| [[_maxTimestampSoFar]]

Used when...FIXME

| _offsetOfMaxTimestampSoFar
a| [[_offsetOfMaxTimestampSoFar]]

Used when...FIXME

|===
