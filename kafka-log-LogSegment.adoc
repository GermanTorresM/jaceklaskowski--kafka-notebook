== [[LogSegment]] LogSegment

`LogSegment` is a segment of a <<kafka-log-Log.adoc#, partition log>>.

`LogSegment` has three components: a <<log, log>> file with <<lazyOffsetIndex, index>> and <<lazyTimeIndex, time>> index files. With a given <<baseOffset, base offset>> the files would be: a `[base offset].index`, a `[base offset].log`, a `[base offset].timeindex` files, e.g.

```
$ tree /tmp/kafka-logs/t1-1/
/tmp/kafka-logs/t1-1/
├── 00000000000000000000.index
├── 00000000000000000000.log
├── 00000000000000000000.timeindex
└── leader-epoch-checkpoint
```

`LogSegment` is <<creating-instance, created>> (indirectly using <<open, open>>) when:

* `Log` is requested to <<kafka-log-Log.adoc#loadSegmentFiles, loadSegmentFiles>> (for every log file), <<kafka-log-Log.adoc#completeSwapOperations, completeSwapOperations>>, <<kafka-log-Log.adoc#loadSegments, loadSegments>>, <<kafka-log-Log.adoc#recoverLog, recoverLog>>, <<kafka-log-Log.adoc#roll, roll>>, and <<kafka-log-Log.adoc#truncateFullyAndStartAt, truncateFullyAndStartAt>>

* `LogCleaner` is requested to <<kafka-log-LogCleaner.adoc#createNewCleanedSegment, createNewCleanedSegment>>

=== [[creating-instance]] Creating LogSegment Instance

`LogSegment` takes the following to be created:

* [[log]] `FileRecords`
* [[lazyOffsetIndex]] `LazyOffsetIndex`
* [[lazyTimeIndex]] `LazyTimeIndex`
* [[txnIndex]] `TransactionIndex`
* [[baseOffset]] Base offset
* [[indexIntervalBytes]] `indexIntervalBytes`
* [[rollJitterMs]] `rollJitterMs`
* [[time]] `Time`

`LogSegment` initializes the <<internal-properties, internal properties>>.

=== [[recover]] `recover` Method

[source, scala]
----
recover(
  producerStateManager: ProducerStateManager,
  leaderEpochCache: Option[LeaderEpochFileCache] = None): Int
----

`recover`...FIXME

NOTE: `recover` is used exclusively when `Log` is requested to <<kafka-log-Log.adoc#recoverSegment, recover a log segment>>.

=== [[sanityCheck]] `sanityCheck` Method

[source, scala]
----
sanityCheck(
  timeIndexFileNewlyCreated: Boolean): Unit
----

`sanityCheck`...FIXME

NOTE: `sanityCheck` is used exclusively when `Log` is requested to <<kafka-log-Log.adoc#loadSegments, loadSegments>> (when <<kafka-log-Log.adoc#creating-instance-loadSegments, created>>).

=== [[open]] Creating LogSegment Instance -- `open` Utility

[source, scala]
----
open(
  dir: File,
  baseOffset: Long,
  config: LogConfig,
  time: Time,
  fileAlreadyExists: Boolean = false,
  initFileSize: Int = 0,
  preallocate: Boolean = false,
  fileSuffix: String = ""): LogSegment
----

`open` uses the following configuration properties (of the given <<kafka-log-LogConfig.adoc#, LogConfig>>):

* <<kafka-log-LogConfig.adoc#maxIndexSize, segment.index.bytes>> for the `maxIndexSize`

* <<kafka-log-LogConfig.adoc#indexInterval, index.interval.bytes>> for the `indexIntervalBytes`

* <<kafka-log-LogConfig.adoc#randomSegmentJitter, segment.index.bytes>> for the `rollJitterMs`

`open` creates a new <<creating-instance, LogSegment>> with the following files in the given `dir` directory:

* <<kafka-log-Log.adoc#logFile, Creates a log file>> (with the given `baseOffset`, and `fileSuffix`) and opens it (using `FileRecords.open`)

* <<kafka-log-Log.adoc#offsetIndexFile, Creates an offset index file>>

* <<kafka-log-Log.adoc#timeIndexFile, Creates a time index file>>

* <<kafka-log-Log.adoc#transactionIndexFile, Creates a transaction index file>>

[NOTE]
====
`open` is used when:

* `Log` is requested to <<kafka-log-Log.adoc#loadSegmentFiles, loadSegmentFiles>> (for every log file), <<kafka-log-Log.adoc#completeSwapOperations, completeSwapOperations>>, <<kafka-log-Log.adoc#loadSegments, loadSegments>>, <<kafka-log-Log.adoc#recoverLog, recoverLog>>, <<kafka-log-Log.adoc#roll, roll>>, and <<kafka-log-Log.adoc#truncateFullyAndStartAt, truncateFullyAndStartAt>>

* `LogCleaner` is requested to <<kafka-log-LogCleaner.adoc#createNewCleanedSegment, createNewCleanedSegment>>
====

=== [[deleteIfExists]] `deleteIfExists` Utility

[source, scala]
----
deleteIfExists(
  dir: File,
  baseOffset: Long,
  fileSuffix: String = ""): Unit
----

`deleteIfExists`...FIXME

NOTE: `deleteIfExists` is used when...FIXME

=== [[resizeIndexes]] `resizeIndexes` Method

[source, scala]
----
resizeIndexes(size: Int): Unit
----

`resizeIndexes`...FIXME

NOTE: `resizeIndexes` is used when...FIXME

=== [[largestTimestamp]] `largestTimestamp` Method

[source, scala]
----
largestTimestamp: Long
----

`largestTimestamp`...FIXME

NOTE: `largestTimestamp` is used when...FIXME

=== [[shouldRoll]] `shouldRoll` Method

[source, scala]
----
shouldRoll(
  rollParams: RollParams): Boolean
----

`shouldRoll`...FIXME

NOTE: `shouldRoll` is used exclusively when `Log` is requested to <<kafka-log-Log.adoc#maybeRoll, maybeRoll>> (while <<kafka-log-Log.adoc#append, appending records>>).

=== [[timeWaitedForRoll]] `timeWaitedForRoll` Method

[source, scala]
----
timeWaitedForRoll(
  now: Long,
  messageTimestamp: Long) : Long
----

`timeWaitedForRoll`...FIXME

NOTE: `timeWaitedForRoll` is used exclusively when `LogSegment` is requested to <<shouldRoll, shouldRoll>>.

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| created
a| [[created]] Time(stamp) when this `LogSegment` was <<creating-instance, created>> or <<truncateTo, truncated completely>> (to `0`)

Used exclusively when `LogSegment` is requested for the <<timeWaitedForRoll, time it has waited to be rolled>>

| bytesSinceLastIndexEntry
a| [[bytesSinceLastIndexEntry]]

Used when...FIXME

| rollingBasedTimestamp
a| [[rollingBasedTimestamp]]

Used when...FIXME

| _maxTimestampSoFar
a| [[_maxTimestampSoFar]]

Used when...FIXME

| _offsetOfMaxTimestampSoFar
a| [[_offsetOfMaxTimestampSoFar]]

Used when...FIXME

|===
