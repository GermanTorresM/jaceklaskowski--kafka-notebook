== [[LogManager]] LogManager

`LogManager` is <<creating-instance, created>> and immediately <<startup, started>> when `KafkaServer` is requested to <<kafka-server-KafkaServer.adoc#startup, start up>>.

.LogManager and KafkaServer
image::images/LogManager.png[align="center"]

While being <<creating-instance, created>>, `LogManager` is given the <<logDirs, log directories>> that are configured using <<kafka-server-KafkaConfig.adoc#logDirs, log.dirs or log.dir>> configuration properties (default: <<kafka-properties.adoc#log.dir, /tmp/kafka-logs>>). The log directories are immediately <<createAndValidateLogDirs, validated>> and <<loadLogs, loaded>>.

`LogManager` uses the <<kafka-properties.adoc#num.recovery.threads.per.data.dir, num.recovery.threads.per.data.dir>> dynamic configuration (default: `1`) for the number of threads per log data directory for <<loadLogs, log recovery at startup>> and <<shutdown, flushing at shutdown>>.

`LogManager` is used to create a <<kafka-server-ReplicaManager.adoc#logManager, ReplicaManager>>, a <<kafka-server-DynamicLogConfig.adoc#logManager, DynamicLogConfig>>, a <<kafka-server-TopicConfigHandler.adoc#logManager, TopicConfigHandler>>.

[[InitialTaskDelayMs]]
`LogManager` defaults to `30000ms` for the *initial task delay*.

[[logging]]
[TIP]
====
Enable `ALL` logging level for `kafka.log.LogManager` logger to see what happens inside.

Add the following line to `config/log4j.properties`:

```
log4j.logger.kafka.log.LogManager=ALL
```

Refer to <<kafka-logging.adoc#, Logging>>.
====

=== [[creating-instance]] Creating LogManager Instance

`LogManager` takes the following to be created:

* [[logDirs]] Absolute paths to log directories (`Seq[File]`)
* [[initialOfflineDirs]] Initial offline directories (`Seq[File]`)
* [[topicConfigs]] Topic configurations (`Map[String, LogConfig]`)
* [[initialDefaultConfig]] Initial <<kafka-log-LogConfig.adoc#, LogConfig>>
* [[cleanerConfig]] <<kafka-log-LogCleaner.adoc#CleanerConfig, CleanerConfig>>
* [[recoveryThreadsPerDataDir]] `recoveryThreadsPerDataDir` (based on the <<kafka-server-KafkaConfig.adoc#numRecoveryThreadsPerDataDir, num.recovery.threads.per.data.dir>> dynamic configuration property)
* [[flushCheckMs]] `flushCheckMs`
* [[flushRecoveryOffsetCheckpointMs]] `flushRecoveryOffsetCheckpointMs`
* [[flushStartOffsetCheckpointMs]] `flushStartOffsetCheckpointMs`
* [[retentionCheckMs]] `retentionCheckMs`
* [[maxPidExpirationMs]] `maxPidExpirationMs`
* [[scheduler]] <<kafka-Scheduler.adoc#, Scheduler>>
* [[brokerState]] `BrokerState`
* [[brokerTopicStats]] <<kafka-server-BrokerTopicStats.adoc#, BrokerTopicStats>>
* [[logDirFailureChannel]] `LogDirFailureChannel`
* [[time]] `Time`

`LogManager` initializes the <<internal-properties, internal properties>>.

While being created, `LogManager` <<loadLogs, loads logs>>.

=== [[KafkaMetricsGroup]][[metrics]] Performance Metrics

`LogManager` is a <<kafka-metrics-KafkaMetricsGroup.adoc#, KafkaMetricsGroup>> with the following performance metrics.

.LogManager's Performance Metrics
[cols="30m,70",options="header",width="100%"]
|===
| Metric Name
| Description

| OfflineLogDirectoryCount
| [[offlineLogDirectoryCount]][[OfflineLogDirectoryCount]] The number of offline log directories

| LogDirectoryOffline
a| [[LogDirectoryOffline]] Registered for every <<logDirs, log directory>> to indicate whether it is online or offline

Possible values:

* `0` when a log directory is online

* `1` when a log directory is offline

The path of the directory is the tag of the metric.

|===

The performance metrics are registered in *kafka.log:type=LogManager* group.

.LogManager in jconsole
image::images/LogManager-jconsole.png[align="center"]

=== [[startup]] Starting Up -- `startup` Method

[source, scala]
----
startup(): Unit
----

`startup` starts the background threads to flush logs and do log cleanup.

Internally, `startup` prints out the following INFO message to the logs:

```
Starting log cleanup with a period of [retentionCheckMs] ms.
```

`startup` requests the <<scheduler, Scheduler>> to <<kafka-Scheduler.adoc#schedule, schedule a task>> with the name *kafka-log-retention* that <<cleanupLogs, cleanupLogs>> with the <<InitialTaskDelayMs, InitialTaskDelayMs>> delay and the <<retentionCheckMs, retentionCheckMs>> execution period.

`startup` prints out the following INFO message to the logs:

```
Starting log flusher with a default period of [flushCheckMs] ms.
```

`startup` requests the <<scheduler, Scheduler>> to <<kafka-Scheduler.adoc#schedule, schedule a task>> with the name *kafka-log-flusher* that <<flushDirtyLogs, flushDirtyLogs>> with the <<InitialTaskDelayMs, InitialTaskDelayMs>> delay and the <<flushCheckMs, flushCheckMs>> execution period.

`startup` requests the <<scheduler, Scheduler>> to <<kafka-Scheduler.adoc#schedule, schedule a task>> with the name *kafka-recovery-point-checkpoint* that <<checkpointLogRecoveryOffsets, checkpointLogRecoveryOffsets>> with the <<InitialTaskDelayMs, InitialTaskDelayMs>> delay and the <<flushRecoveryOffsetCheckpointMs, flushRecoveryOffsetCheckpointMs>> execution period.

`startup` requests the <<scheduler, Scheduler>> to <<kafka-Scheduler.adoc#schedule, schedule a task>> with the name *kafka-log-start-offset-checkpoint* that <<checkpointLogStartOffsets, checkpointLogStartOffsets>> with the <<InitialTaskDelayMs, InitialTaskDelayMs>> delay and the <<flushStartOffsetCheckpointMs, flushStartOffsetCheckpointMs>> execution period.

`startup` requests the <<scheduler, Scheduler>> to <<kafka-Scheduler.adoc#schedule, schedule a task>> with the name *kafka-delete-logs* that <<deleteLogs, deleteLogs>> with the <<InitialTaskDelayMs, InitialTaskDelayMs>> delay.

(only when the <<cleanerConfig, CleanerConfig>> has the <<kafka-log-LogCleaner.adoc#enableCleaner, enableCleaner>> flag enabled) `startup` requests the <<cleaner, LogCleaner>> to <<kafka-log-LogCleaner.adoc#startup, start up>>.

NOTE: `startup` is used exclusively when `KafkaServer` is requested to <<kafka-server-KafkaServer.adoc#startup, start up>>.

=== [[cleanupLogs]] `cleanupLogs` Method

[source, scala]
----
cleanupLogs(): Unit
----

`cleanupLogs`...FIXME

NOTE: `cleanupLogs` is used when...FIXME

=== [[allLogs]] Getting All Partition Logs -- `allLogs` Method

[source, scala]
----
allLogs: Iterable[Log]
----

`allLogs`...FIXME

NOTE: `allLogs` is used when...FIXME

=== [[addLogToBeDeleted]] `addLogToBeDeleted` Internal Method

[source, scala]
----
addLogToBeDeleted(log: Log): Unit
----

`addLogToBeDeleted`...FIXME

NOTE: `addLogToBeDeleted` is used when...FIXME

=== [[asyncDelete]] `asyncDelete` Method

[source, scala]
----
asyncDelete(
  topicPartition: TopicPartition,
  isFuture: Boolean = false): Log
----

`asyncDelete`...FIXME

[NOTE]
====
`asyncDelete` is used when:

* `Partition` is requested to <<kafka-cluster-Partition.adoc#removeFutureLocalReplica, removeFutureLocalReplica>> and <<kafka-cluster-Partition.adoc#delete, delete>>

* `ReplicaManager` is requested to <<kafka-server-ReplicaManager.adoc#stopReplica, stopReplica>>
====

=== [[getOrCreateLog]] `getOrCreateLog` Method

[source, scala]
----
getOrCreateLog(
  topicPartition: TopicPartition,
  config: LogConfig,
  isNew: Boolean = false,
  isFuture: Boolean = false): Log
----

`getOrCreateLog`...FIXME

NOTE: `getOrCreateLog` is used exclusively when `Partition` is requested to <<kafka-cluster-Partition.adoc#getOrCreateReplica, getOrCreateReplica>>.

=== [[loadLog]] `loadLog` Internal Method

[source, scala]
----
loadLog(
  logDir: File,
  recoveryPoints: Map[TopicPartition, Long],
  logStartOffsets: Map[TopicPartition, Long]): Unit
----

`loadLog`...FIXME

NOTE: `loadLog` is used exclusively when `LogManager` is requested to <<loadLogs, loadLogs>>.

=== [[loadLogs]] Loading Logs -- `loadLogs` Internal Method

[source, scala]
----
loadLogs(): Unit
----

`loadLogs` prints out the following INFO message to the logs:

```
Loading logs.
```

For every <<liveLogDirs, live log directory>>, `loadLogs`...FIXME

NOTE: `loadLogs` is used exclusively when `LogManager` is <<creating-instance, created>>.

=== [[apply]] Creating LogManager -- `apply` Factory Method

[source, scala]
----
apply(
  config: KafkaConfig,
  initialOfflineDirs: Seq[String],
  zkClient: KafkaZkClient,
  brokerState: BrokerState,
  kafkaScheduler: KafkaScheduler,
  time: Time,
  brokerTopicStats: BrokerTopicStats,
  logDirFailureChannel: LogDirFailureChannel): LogManager
----

`apply`...FIXME

NOTE: `apply` is used exclusively when `KafkaServer` is requested to <<kafka-server-KafkaServer.adoc#startup, start up>>.

=== [[liveLogDirs]] `liveLogDirs` Method

[source, scala]
----
liveLogDirs: Seq[File]
----

`liveLogDirs`...FIXME

NOTE: `liveLogDirs` is used when...FIXME

=== [[deleteLogs]] `deleteLogs` Internal Method

[source, scala]
----
deleteLogs(): Unit
----

`deleteLogs`...FIXME

NOTE: `deleteLogs` is used when...FIXME

=== [[flushDirtyLogs]] `flushDirtyLogs` Internal Method

[source, scala]
----
flushDirtyLogs(): Unit
----

`flushDirtyLogs`...FIXME

NOTE: `flushDirtyLogs` is used when...FIXME

=== [[checkpointLogRecoveryOffsets]] `checkpointLogRecoveryOffsets` Method

[source, scala]
----
checkpointLogRecoveryOffsets(): Unit
----

`checkpointLogRecoveryOffsets`...FIXME

NOTE: `checkpointLogRecoveryOffsets` is used when...FIXME

=== [[checkpointLogStartOffsets]] `checkpointLogStartOffsets` Method

[source, scala]
----
checkpointLogStartOffsets(): Unit
----

`checkpointLogStartOffsets`...FIXME

NOTE: `checkpointLogStartOffsets` is used when...FIXME

=== [[isLogDirOnline]] `isLogDirOnline` Method

[source, scala]
----
isLogDirOnline(logDir: String): Boolean
----

`isLogDirOnline`...FIXME

NOTE: `isLogDirOnline` is used when...FIXME

=== [[createAndValidateLogDirs]] Validating Data Log Directories -- `createAndValidateLogDirs` Internal Method

[source, scala]
----
createAndValidateLogDirs(
  dirs: Seq[File],
  initialOfflineDirs: Seq[File]): ConcurrentLinkedQueue[File]
----

For every directory in the given `dirs`, `createAndValidateLogDirs` makes sure that the data directory is available (i.e. it is a readable directory) or creates it.

`createAndValidateLogDirs` prints out the following INFO message to the logs when a data directory does not exist:

```
Log directory [dir] not found, creating it.
```

NOTE: `createAndValidateLogDirs` is given the <<logDirs, logDirs>> and the <<initialOfflineDirs, initialOfflineDirs>> that `LogManager` is <<creating-instance, created>> with.

`createAndValidateLogDirs` throws...FIXME

NOTE: `createAndValidateLogDirs` is used exclusively when `LogManager` is <<_liveLogDirs, created>>.

=== [[truncateTo]] `truncateTo` Method

[source, scala]
----
truncateTo(
  partitionOffsets: Map[TopicPartition, Long],
  isFuture: Boolean): Unit
----

`truncateTo`...FIXME

NOTE: `truncateTo` is used exclusively when `Partition` is requested to <<kafka-cluster-Partition.adoc#truncateTo, truncateTo>>.

=== [[truncateFullyAndStartAt]] `truncateFullyAndStartAt` Method

[source, scala]
----
truncateFullyAndStartAt(
  topicPartition: TopicPartition,
  newOffset: Long,
  isFuture: Boolean): Unit
----

`truncateFullyAndStartAt`...FIXME

NOTE: `truncateFullyAndStartAt` is used exclusively when `Partition` is requested to <<kafka-cluster-Partition.adoc#truncateFullyAndStartAt, truncateFullyAndStartAt>>.

=== [[resizeRecoveryThreadPool]] `resizeRecoveryThreadPool` Method

[source, scala]
----
resizeRecoveryThreadPool(newSize: Int): Unit
----

`resizeRecoveryThreadPool` prints out the following INFO message to the logs and reconfigures the <<numRecoveryThreadsPerDataDir, numRecoveryThreadsPerDataDir>> internal registry to be the given `newSize`.

```
Resizing recovery thread pool size for each data dir from [numRecoveryThreadsPerDataDir] to [newSize]
```

NOTE: `resizeRecoveryThreadPool` is used exclusively when `DynamicThreadPool` is requested to <<kafka-server-DynamicThreadPool.adoc#reconfigure, reconfigure>> (with a new value of <<kafka-server-KafkaConfig.adoc#numRecoveryThreadsPerDataDir, KafkaConfig.numRecoveryThreadsPerDataDir>>).

=== [[shutdown]] Shutting Down -- `shutdown` Method

[source, scala]
----
shutdown(): Unit
----

`shutdown` prints out the following INFO message to the logs:

```
Shutting down.
```

`shutdown` then...FIXME

NOTE: `shutdown` is used exclusively when `KafkaServer` is requested to <<kafka-server-KafkaServer.adoc#shutdown, shutdown>>.

=== [[replaceCurrentWithFutureLog]] `replaceCurrentWithFutureLog` Method

[source, scala]
----
replaceCurrentWithFutureLog(topicPartition: TopicPartition): Unit
----

`replaceCurrentWithFutureLog`...FIXME

NOTE: `replaceCurrentWithFutureLog` is used exclusively when `Partition` is requested to <<kafka-cluster-Partition.adoc#maybeReplaceCurrentWithFutureReplica, maybeReplaceCurrentWithFutureReplica>>.

=== [[handleLogDirFailure]] `handleLogDirFailure` Method

[source, scala]
----
handleLogDirFailure(dir: String): Unit
----

`handleLogDirFailure`...FIXME

NOTE: `handleLogDirFailure` is used exclusively when `ReplicaManager` is requested to <<kafka-server-ReplicaManager.adoc#handleLogDirFailure, handleLogDirFailure>>.

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| _liveLogDirs
a| [[_liveLogDirs]] Java's https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/ConcurrentLinkedQueue.html[ConcurrentLinkedQueue] of live log directories (after <<createAndValidateLogDirs, createAndValidateLogDirs>> was executed with the <<logDirs, logDirs>> and the <<initialOfflineDirs, initialOfflineDirs>> directories).

Used when...FIXME

| cleaner
a| [[cleaner]]

| numRecoveryThreadsPerDataDir
a| [[numRecoveryThreadsPerDataDir]] Number of recovery threads per log data directory

Starts as the <<recoveryThreadsPerDataDir, recoveryThreadsPerDataDir>> and can then be <<resizeRecoveryThreadPool, dynamically changed>>.

|===
