== [[Log]] Partition Log

`Log` is a *partition log* that is <<apply, created>> when `LogManager` is requested to <<kafka-log-LogManager.adoc#loadLog, loadLog>> (when `LogManager` is <<kafka-log-LogManager.adoc#creating-instance, created>>) and <<kafka-log-LogManager.adoc#getOrCreateLog, getOrCreateLog>>.

[[CleanShutdownFile]]
`Log` uses *.kafka_cleanshutdown* file to indicate...FIXME

[[isFuture]]
`Log` is *isFuture* when...FIXME

[[suffixes]]
.File Suffixes
[cols="30m,70",options="header",width="100%"]
|===
| Suffix
| Description

| -delete
a| [[DeleteDirSuffix]]

| .txnindex
a| [[TxnIndexFileSuffix]] <<isIndexFile, Index file>>

| .timeindex
a| [[TimeIndexFileSuffix]] <<isIndexFile, Index file>>

| .index
a| [[IndexFileSuffix]] <<isIndexFile, Index file>>

| .log
a| [[LogFileSuffix]] <<isLogFile, Log file>>

|===

`Log` uses a <<scheduler, Scheduler>> to schedule the <<background-tasks, background tasks>>.

[[background-tasks]]
.Log's Background Tasks
[cols="1m,1,1,2",options="header",width="100%"]
|===
| Name
| Period
| Delay
| Description

| PeriodicProducerExpirationCheck
| <<producerIdExpirationCheckIntervalMs, producerIdExpirationCheckIntervalMs>>
| <<producerIdExpirationCheckIntervalMs, producerIdExpirationCheckIntervalMs>>
| [[PeriodicProducerExpirationCheck]] Requests the <<producerStateManager, ProducerStateManager>> to <<kafka-log-ProducerStateManager.adoc#removeExpiredProducers, removeExpiredProducers>>

Scheduled immediately when `Log` is <<creating-instance, created>>.

| flush-log
| `-1` (once)
| `0L`
| [[flush-log]] <<flush, flush>>

Scheduled when `Log` is requested to <<roll, roll>>.

| delete-file
| `-1` (once)
| <<kafka-log-LogConfig.adoc#fileDeleteDelayMs, file.delete.delay.ms>>
| [[delete-file]] <<deleteSeg, deleteSeg>>

Scheduled when `Log` is requested to <<asyncDeleteSegment, asyncDeleteSegment>>.

|===

[[logIdent]]
`Log` uses *[Log partition=[topicPartition], dir=[parent]]* as the logging prefix (aka `logIdent`).

[[logging]]
[TIP]
====
Enable `ALL` logging levels for `kafka.log.Log` logger to see what happens inside.

Add the following line to `log4j.properties`:

```
log4j.logger.kafka.log.Log=ALL
```

Refer to <<kafka-logging.adoc#, Logging>>.
====

=== [[KafkaMetricsGroup]][[metrics]] Performance Metrics

`Log` is a <<kafka-metrics-KafkaMetricsGroup.adoc#, KafkaMetricsGroup>> with the following performance metrics.

.Log's Performance Metrics
[cols="30m,70",options="header",width="100%"]
|===
| Metric Name
| Description

| NumLogSegments
| [[NumLogSegments]]

| LogEndOffset
| [[LogEndOffset-metrics]]

| LogStartOffset
| [[LogStartOffset]]

| Size
| [[Size]]

|===

The performance metrics are registered in *kafka.log:type=Log* group.

.Log in jconsole
image::images/Log-jconsole.png[align="center"]

=== [[creating-instance]] Creating Log Instance

`Log` takes the following to be created:

* [[dir]] Log directory
* [[config]] <<kafka-log-LogConfig.adoc#, LogConfig>>
* [[logStartOffset]] `logStartOffset`
* [[recoveryPoint]] `recoveryPoint`
* [[scheduler]] <<kafka-Scheduler.adoc#, Scheduler>>
* [[brokerTopicStats]] <<kafka-server-BrokerTopicStats.adoc#, BrokerTopicStats>>
* [[time]] `Time`
* [[maxProducerIdExpirationMs]] `maxProducerIdExpirationMs`
* [[producerIdExpirationCheckIntervalMs]] `producerIdExpirationCheckIntervalMs`
* [[topicPartition]] `TopicPartition`
* [[producerStateManager]] `ProducerStateManager`
* [[logDirFailureChannel]] `LogDirFailureChannel`

`Log` initializes the <<internal-properties, internal properties>>.

[[creating-instance-nextOffsetMetadata]]
While being created, `Log`...FIXME

[[creating-instance-loadProducerState]]
`Log` <<loadProducerState, loadProducerState>> (with the `logEndOffset` and the `reloadFromCleanShutdown` based on <<hasCleanShutdownFile, hasCleanShutdownFile>>).

In the end, `Log` prints out the following INFO message to the logs:

[options="wrap"]
----
Completed load of log with [size] segments, log start offset [logStartOffset] and log end offset [logEndOffset] in [time] ms
----

=== [[apply]] Creating Log Instance -- `apply` Factory Method

[source, scala]
----
apply(
  dir: File,
  config: LogConfig,
  logStartOffset: Long,
  recoveryPoint: Long,
  scheduler: Scheduler,
  brokerTopicStats: BrokerTopicStats,
  time: Time = Time.SYSTEM,
  maxProducerIdExpirationMs: Int,
  producerIdExpirationCheckIntervalMs: Int,
  logDirFailureChannel: LogDirFailureChannel): Log
----

`apply`...FIXME

NOTE: `apply` is used when `LogManager` is requested to <<kafka-log-LogManager.adoc#loadLog, loadLog>> and <<kafka-log-LogManager.adoc#getOrCreateLog, getOrCreateLog>>.

=== [[roll]] `roll` Method

[source, scala]
----
roll(
  expectedNextOffset: Option[Long] = None): LogSegment
----

`roll`...FIXME

NOTE: `roll` is used when `Log` is requested to <<deleteSegments, deleteSegments>> and <<maybeRoll, maybeRoll>>.

=== [[maybeRoll]] `maybeRoll` Internal Method

[source, scala]
----
maybeRoll(
  messagesSize: Int,
  appendInfo: LogAppendInfo): LogSegment
----

`maybeRoll`...FIXME

NOTE: `maybeRoll` is used exclusively when `Log` is requested to <<append, append>>.

=== [[asyncDeleteSegment]] `asyncDeleteSegment` Internal Method

[source, scala]
----
asyncDeleteSegment(segment: LogSegment): Unit
----

`asyncDeleteSegment`...FIXME

NOTE: `asyncDeleteSegment` is used when `Log` is requested to <<deleteSegment, deleteSegment>> and <<replaceSegments, replaceSegments>>.

=== [[flush]] `flush` Method

[source, scala]
----
flush(): Unit // <1>
flush(offset: Long): Unit
----
<1> Uses <<logEndOffset, logEndOffset>> for the offset

`flush`...FIXME

[NOTE]
====
`flush` is used when:

* `Log` is requested to <<append, append records>> and for the <<flush-log, flush-log Background Task>>

* `LogManager` is requested to <<kafka-log-LogManager.adoc#shutdown, shut down>> and <<kafka-log-LogManager.adoc#flushDirtyLogs, flushDirtyLogs>>
====

=== [[deleteSeg]] `deleteSeg` Internal Method

[source, scala]
----
deleteSeg(): Unit
----

`deleteSeg`...FIXME

NOTE: `deleteSeg` is used exclusively for the <<delete-file, delete-file Background Task>>.

=== [[appendAsLeader]] `appendAsLeader` Method

[source, scala]
----
appendAsLeader(
  records: MemoryRecords,
  leaderEpoch: Int,
  isFromClient: Boolean = true): LogAppendInfo
----

`appendAsLeader` simply <<append, append>> with the `assignOffsets` flag on.

NOTE: `appendAsLeader` is used exclusively when `Partition` is requested to <<kafka-cluster-Partition.adoc#appendRecordsToLeader, appendRecordsToLeader>>.

=== [[appendAsFollower]] `appendAsFollower` Method

[source, scala]
----
appendAsFollower(records: MemoryRecords): LogAppendInfo
----

`appendAsFollower` simply <<append, append>> (with the `isFromClient` and `assignOffsets` flags off, and the `leaderEpoch` being `-1`).

NOTE: `appendAsFollower` is used exclusively when `Partition` is requested to <<kafka-cluster-Partition.adoc#doAppendRecordsToFollowerOrFutureReplica, doAppendRecordsToFollowerOrFutureReplica>>.

=== [[append]] `append` Internal Method

[source, scala]
----
append(
  records: MemoryRecords,
  isFromClient: Boolean,
  interBrokerProtocolVersion: ApiVersion,
  assignOffsets: Boolean,
  leaderEpoch: Int): LogAppendInfo
----

`append`...FIXME

NOTE: `append` is used when `Log` is requested to <<appendAsLeader, appendAsLeader>> (with `assignOffsets` flag on) and <<appendAsFollower, appendAsFollower>> (with `assignOffsets` and `isFromClient` flags off).

==== [[analyzeAndValidateRecords]] `analyzeAndValidateRecords` Internal Method

[source, scala]
----
analyzeAndValidateRecords(
  records: MemoryRecords,
  isFromClient: Boolean): LogAppendInfo
----

`analyzeAndValidateRecords`...FIXME

NOTE: `analyzeAndValidateRecords` is used exclusively when `Log` is requested to <<append, append>>.

=== [[deleteSegment]] `deleteSegment` Internal Method

[source, scala]
----
deleteSegment(segment: LogSegment): Unit
----

`deleteSegment`...FIXME

NOTE: `deleteSegment` is used when `Log` is requested to <<recoverLog, recoverLog>>, <<deleteSegments, deleteSegments>>, <<roll, roll>>, <<truncateTo, truncateTo>>, and <<truncateFullyAndStartAt, truncateFullyAndStartAt>>.

=== [[replaceSegments]] `replaceSegments` Internal Method

[source, scala]
----
replaceSegments(
  newSegments: Seq[LogSegment],
  oldSegments: Seq[LogSegment],
  isRecoveredSwapFile: Boolean = false): Unit
----

`replaceSegments`...FIXME

[NOTE]
====
`replaceSegments` is used when:

* `Log` is requested to <<completeSwapOperations, completeSwapOperations>> and <<splitOverflowedSegment, splitOverflowedSegment>>

* `Cleaner` is requested to `cleanSegments`
====

=== [[recoverLog]] `recoverLog` Internal Method

[source, scala]
----
recoverLog(): Long
----

`recoverLog`...FIXME

NOTE: `recoverLog` is used exclusively when `Log` is requested to <<loadSegments, loadSegments>>.

=== [[hasCleanShutdownFile]] Checking Whether .kafka_cleanshutdown Is In Parent Directory of Log Directory -- `hasCleanShutdownFile` Internal Method

[source, scala]
----
hasCleanShutdownFile: Boolean
----

`hasCleanShutdownFile` is `true` when <<CleanShutdownFile, .kafka_cleanshutdown>> file is in the parent directory of the <<dir, log directory>>. Otherwise, `hasCleanShutdownFile` is `false`.

NOTE: `hasCleanShutdownFile` is used exclusively when `Log` is <<creating-instance, created>> (to <<loadProducerState, loadProducerState>>) and requested to <<recoverLog, recoverLog>>.

=== [[deleteSegments]] `deleteSegments` Internal Method

[source, scala]
----
deleteSegments(deletable: Iterable[LogSegment]): Int
----

`deleteSegments`...FIXME

NOTE: `deleteSegments` is used exclusively when `Log` is requested to <<deleteOldSegments, deleteOldSegments>>.

=== [[truncateTo]] `truncateTo` Internal Method

[source, scala]
----
truncateTo(targetOffset: Long): Boolean
----

`truncateTo`...FIXME

NOTE: `truncateTo` is used exclusively when `LogManager` is requested to <<kafka-log-LogManager.adoc#truncateTo, truncateTo>>.

=== [[truncateFullyAndStartAt]] `truncateFullyAndStartAt` Internal Method

[source, scala]
----
truncateFullyAndStartAt(newOffset: Long): Unit
----

`truncateFullyAndStartAt`...FIXME

[NOTE]
====
`truncateFullyAndStartAt` is used when:

* `Log` is requested to <<truncateTo, truncateTo>>

* `LogManager` is requested to <<kafka-log-LogManager.adoc#truncateFullyAndStartAt, truncateFullyAndStartAt>>
====

=== [[deleteOldSegments]] `deleteOldSegments` Method

[source, scala]
----
deleteOldSegments(): Long
// Private API
deleteOldSegments(
  predicate: (LogSegment, Option[LogSegment]) => Boolean,
  reason: String): Int
----

`deleteOldSegments`...FIXME

[NOTE]
====
`deleteOldSegments` is used when:

* `CleanerThread` (of <<kafka-log-LogCleaner.adoc#, LogCleaner>>) is requested to <<kafka-log-CleanerThread.adoc#cleanFilthiestLog, cleanFilthiestLog>>

* `LogManager` is requested to <<kafka-log-LogManager.adoc#cleanupLogs, cleanupLogs>>

* `Log` is requested to <<deleteRetentionMsBreachedSegments, deleteRetentionMsBreachedSegments>>, <<deleteRetentionSizeBreachedSegments, deleteRetentionSizeBreachedSegments>>, and <<deleteLogStartOffsetBreachedSegments, deleteLogStartOffsetBreachedSegments>>
====

=== [[deleteRetentionMsBreachedSegments]] `deleteRetentionMsBreachedSegments` Internal Method

[source, scala]
----
deleteRetentionMsBreachedSegments(): Int
----

`deleteRetentionMsBreachedSegments`...FIXME

NOTE: `deleteRetentionMsBreachedSegments` is used when...FIXME

=== [[deleteRetentionSizeBreachedSegments]] `deleteRetentionSizeBreachedSegments` Internal Method

[source, scala]
----
deleteRetentionSizeBreachedSegments(): Int
----

`deleteRetentionSizeBreachedSegments`...FIXME

NOTE: `deleteRetentionSizeBreachedSegments` is used when...FIXME

=== [[deleteLogStartOffsetBreachedSegments]] `deleteLogStartOffsetBreachedSegments` Internal Method

[source, scala]
----
deleteLogStartOffsetBreachedSegments(): Int
----

`deleteLogStartOffsetBreachedSegments`...FIXME

NOTE: `deleteLogStartOffsetBreachedSegments` is used when...FIXME

=== [[splitOverflowedSegment]] `splitOverflowedSegment` Internal Method

[source, scala]
----
splitOverflowedSegment(segment: LogSegment): List[LogSegment]
----

`splitOverflowedSegment`...FIXME

[NOTE]
====
`splitOverflowedSegment` is used when:

* `Log` is requested to <<retryOnOffsetOverflow, retryOnOffsetOverflow>>

* `LogCleaner` is requested to <<kafka-log-LogCleaner.adoc#cleanSegments, cleanSegments>>
====

=== [[onHighWatermarkIncremented]] `onHighWatermarkIncremented` Method

[source, scala]
----
onHighWatermarkIncremented(highWatermark: Long): Unit
----

`onHighWatermarkIncremented`...FIXME

NOTE: `onHighWatermarkIncremented` is used when `Replica` is <<kafka-cluster-Replica.adoc#, created>> and <<kafka-cluster-Replica.adoc#highWatermark_, highWatermark_=>>.

=== [[parseTopicPartitionName]] `parseTopicPartitionName` Object Method

[source, scala]
----
parseTopicPartitionName(dir: File): TopicPartition
----

`parseTopicPartitionName`...FIXME

[NOTE]
====
`parseTopicPartitionName` is used when:

* `Log` is <<apply, created>>

* `LogManager` is requested to <<kafka-log-LogManager.adoc#loadLog, load a partition log directory>>
====

=== [[offsetFromFileName]] `offsetFromFileName` Object Method

[source, scala]
----
offsetFromFileName(filename: String): Long
----

`offsetFromFileName`...FIXME

NOTE: `offsetFromFileName` is used when `Log` is requested to <<removeTempFilesAndCollectSwapFiles, removeTempFilesAndCollectSwapFiles>> (right when <<creating-instance, created>>) and <<offsetFromFile, offsetFromFile>>.

=== [[offsetFromFile]] `offsetFromFile` Object Method

[source, scala]
----
offsetFromFile(file: File): Long
----

`offsetFromFile`...FIXME

NOTE: `offsetFromFile` is used when...FIXME

=== [[read]] `read` Method

[source, scala]
----
read(
  startOffset: Long,
  maxLength: Int,
  maxOffset: Option[Long],
  minOneMessage: Boolean,
  includeAbortedTxns: Boolean): FetchDataInfo
----

`read`...FIXME

[NOTE]
====
`read` is used when:

* `Partition` is requested to <<kafka-cluster-Partition.adoc#readRecords, readRecords>>

* `GroupMetadataManager` is requested to <<kafka-coordinator-group-GroupMetadataManager.adoc#doLoadGroupsAndOffsets, doLoadGroupsAndOffsets>>

* `TransactionStateManager` is requested to <<kafka-TransactionStateManager.adoc#loadTransactionMetadata, loadTransactionMetadata>>

* `Log` is requested to <<convertToOffsetMetadata, convertToOffsetMetadata>>
====

=== [[convertToOffsetMetadata]] `convertToOffsetMetadata` Method

[source, scala]
----
convertToOffsetMetadata(
  offset: Long): Option[LogOffsetMetadata]
----

`convertToOffsetMetadata`...FIXME

NOTE: `convertToOffsetMetadata` is used exclusively when `Replica` is requested to <<kafka-cluster-Replica.adoc#convertHWToLocalOffsetMetadata, convertHWToLocalOffsetMetadata>>

=== [[logEndOffset]] `logEndOffset` Method

[source, scala]
----
logEndOffset: Long
----

`logEndOffset` is the offset of the next message that will be appended to the log (based on the <<nextOffsetMetadata, nextOffsetMetadata>> internal registry).

NOTE: `logEndOffset` is used when...FIXME

=== [[addSegment]] `addSegment` Method

[source, scala]
----
addSegment(segment: LogSegment): LogSegment
----

`addSegment` simply associates the given <<kafka-log-LogSegment.adoc#, LogSegment>> with the <<kafka-log-LogSegment.adoc#baseOffset, baseOffset>> in the <<segments, segments>> internal registry.

NOTE: `addSegment` is used when `Log` is requested to <<replaceSegments, replaceSegments>>, <<loadSegmentFiles, loadSegmentFiles>>, <<loadSegments, loadSegments>>, <<recoverLog, recoverLog>>, <<roll, roll>>, and <<truncateFullyAndStartAt, truncateFullyAndStartAt>>.

=== [[updateLogEndOffset]] `updateLogEndOffset` Internal Method

[source, scala]
----
updateLogEndOffset(messageOffset: Long): Unit
----

`updateLogEndOffset` simply creates a new `LogOffsetMetadata` (with the `messageOffset`, <<activeSegment, active segment>>) and becomes the <<nextOffsetMetadata, nextOffsetMetadata>> internal registry.

NOTE: `updateLogEndOffset` is used when `Log` is requested to <<append, append records>>, <<roll, roll log segment>>, <<truncateTo, truncateTo>>, and <<truncateFullyAndStartAt, truncateFullyAndStartAt>>.

=== [[activeSegment]] `activeSegment` Method

[source, scala]
----
activeSegment: LogSegment
----

`activeSegment` gives the active <<kafka-log-LogSegment.adoc#, LogSegment>> that is currently taking appends (that is the greatest key in the <<segments, segments>> internal registry).

NOTE: `activeSegment` is used exclusively when `Log` is <<creating-instance, created>> (to create a <<nextOffsetMetadata, LogOffsetMetadata>>).

=== [[loadSegments]] `loadSegments` Internal Method

[source, scala]
----
loadSegments(): Long
----

`loadSegments`...FIXME

NOTE: `loadSegments` is used exclusively when `Log` is <<creating-instance, created>> (to create a <<nextOffsetMetadata, LogOffsetMetadata>>).

=== [[removeTempFilesAndCollectSwapFiles]] `removeTempFilesAndCollectSwapFiles` Internal Method

[source, scala]
----
removeTempFilesAndCollectSwapFiles(): Set[File]
----

`removeTempFilesAndCollectSwapFiles`...FIXME

NOTE: `removeTempFilesAndCollectSwapFiles` is used exclusively when `Log` is requested to <<loadSegments, loadSegments>> (right when <<creating-instance, created>>).

=== [[loadSegmentFiles]] `loadSegmentFiles` Internal Method

[source, scala]
----
loadSegmentFiles(): Unit
----

`loadSegmentFiles` processes <<loadSegmentFiles-isIndexFile, index>> and <<isLogFile, log>> files in the <<dir, log directory>>.

Internally, `loadSegmentFiles` finds all the files (sorted by name) in the <<dir, log directory>> and branches off per whether a file is an <<loadSegmentFiles-isIndexFile, index>> or a <<isLogFile, log>> file.

NOTE: `loadSegmentFiles` is used exclusively when `Log` is requested to <<loadSegments, loadSegments>> (right when <<creating-instance, created>>).

==== [[loadSegmentFiles-isIndexFile]] `loadSegmentFiles` Internal Method and Index Files

For an <<isIndexFile, index file>>, `loadSegmentFiles` simply makes sure that it has a corresponding <<LogFileSuffix, .log>> file (in the same <<dir, log directory>>).

If the file is an orphaned index file, `loadSegmentFiles` simply prints out the following WARN message and deletes the file:

```
Found an orphaned index file [path], with no corresponding log file.
```

==== [[loadSegmentFiles-isLogFile]] `loadSegmentFiles` Internal Method and Log Files

For an <<isLogFile, log file>>, `loadSegmentFiles` <<kafka-log-LogSegment.adoc#open, opens it>> and requests <<kafka-log-LogSegment.adoc#sanityCheck, sanityCheck>>.

In case of `NoSuchFileException`, `loadSegmentFiles` prints out the following ERROR to the logs and <<recoverSegment, recovers the segment>>.

[options="wrap"]
----
Could not find offset index file corresponding to log file [path], recovering segment and rebuilding index files...
----

In case of `CorruptIndexException`, `loadSegmentFiles` prints out the following ERROR to the logs and <<recoverSegment, recovers the segment>>.

[options="wrap"]
----
Found a corrupted index file corresponding to log file [path] due to [message], recovering segment and rebuilding index files...
----

In the end, `loadSegmentFiles` <<addSegment, addSegment>>.

=== [[isIndexFile]] `isIndexFile` Internal Object Method

[source, scala]
----
isIndexFile(file: File): Boolean
----

`isIndexFile` is `true` for files with the following file suffices:

* <<IndexFileSuffix, .index>>

* <<TimeIndexFileSuffix, .timeindex>>

* <<TxnIndexFileSuffix, .txnindex>>

Otherwise, `isIndexFile` is `false`.

NOTE: `isIndexFile` is used when `Log` is requested to <<removeTempFilesAndCollectSwapFiles, removeTempFilesAndCollectSwapFiles>> and <<loadSegmentFiles, loadSegmentFiles>>.

=== [[isLogFile]] `isLogFile` Internal Object Method

[source, scala]
----
isLogFile(file: File): Boolean
----

`isLogFile` returns `true` when the given file has <<LogFileSuffix, .log>> file suffix. Otherwise, `isLogFile` is `false`.

NOTE: `isLogFile` is used when `Log` is requested to <<removeTempFilesAndCollectSwapFiles, removeTempFilesAndCollectSwapFiles>>, <<loadSegmentFiles, loadSegmentFiles>>, and <<splitOverflowedSegment, splitOverflowedSegment>>.

=== [[recoverSegment]] Recovering Log Segment -- `recoverSegment` Internal Method

[source, scala]
----
recoverSegment(
  segment: LogSegment,
  leaderEpochCache: Option[LeaderEpochFileCache] = None): Int
----

`recoverSegment` creates a new <<kafka-log-ProducerStateManager.adoc#, ProducerStateManager>> (for the <<topicPartition, TopicPartition>>, <<dir, log directory>> and <<maxProducerIdExpirationMs, maxProducerIdExpirationMs>>).

NOTE: Why does `recoverSegment` create a new <<kafka-log-ProducerStateManager.adoc#, ProducerStateManager>> rather than using the <<producerStateManager, ProducerStateManager>>?

`recoverSegment` then <<rebuildProducerState, rebuildProducerState>> (with the <<kafka-log-LogSegment.adoc#baseOffset, baseOffset>> of the <<kafka-log-LogSegment.adoc#, LogSegment>>, the `reloadFromCleanShutdown` flag off, and the new `ProducerStateManager`).

`recoverSegment` requests the given `LogSegment` to <<kafka-log-LogSegment.adoc#recover, recover>> (with the new `ProducerStateManager` and the optional `LeaderEpochFileCache`).

`recoverSegment` requests the `ProducerStateManager` to <<kafka-log-ProducerStateManager.adoc#takeSnapshot, takeSnapshot>>.

`recoverSegment` returns the number of bytes truncated from the log (while doing <<kafka-log-LogSegment.adoc#recover, segment recovery>>).

NOTE: `recoverSegment` is used when `Log` is requested to <<loadSegmentFiles, loadSegmentFiles>>, <<completeSwapOperations, completeSwapOperations>>, and <<recoverLog, recoverLog>>.

=== [[loadProducerState]] `loadProducerState` Internal Method

[source, scala]
----
loadProducerState(
  lastOffset: Long,
  reloadFromCleanShutdown: Boolean): Unit
----

`loadProducerState` <<rebuildProducerState, rebuildProducerState>> (with the `lastOffset`, `reloadFromCleanShutdown` and the <<producerStateManager, ProducerStateManager>>).

In the end, `loadProducerState` <<updateFirstUnstableOffset, updateFirstUnstableOffset>>.

NOTE: `loadProducerState` is used when `Log` is <<creating-instance-loadProducerState, created>> and requested to <<truncateTo, truncateTo>>.

=== [[rebuildProducerState]] `rebuildProducerState` Internal Method

[source, scala]
----
rebuildProducerState(
  lastOffset: Long,
  reloadFromCleanShutdown: Boolean,
  producerStateManager: ProducerStateManager): Unit
----

`rebuildProducerState`...FIXME

NOTE: `rebuildProducerState` is used when `Log` is requested to <<recoverSegment, recoverSegment>> and <<loadProducerState, loadProducerState>>.

=== [[updateFirstUnstableOffset]] `updateFirstUnstableOffset` Internal Method

[source, scala]
----
updateFirstUnstableOffset(): Unit
----

`updateFirstUnstableOffset`...FIXME

NOTE: `updateFirstUnstableOffset` is used when `Log` is requested to <<loadProducerState, loadProducerState>>, <<append, append>>, <<onHighWatermarkIncremented, onHighWatermarkIncremented>>, <<maybeIncrementLogStartOffset, maybeIncrementLogStartOffset>>, and <<truncateFullyAndStartAt, truncateFullyAndStartAt>>.

=== [[retryOnOffsetOverflow]] `retryOnOffsetOverflow` Internal Method

[source, scala]
----
retryOnOffsetOverflow[T](fn: => T): T
----

`retryOnOffsetOverflow`...FIXME

NOTE: `retryOnOffsetOverflow` is used when...FIXME

=== [[completeSwapOperations]] `completeSwapOperations` Internal Method

[source, scala]
----
completeSwapOperations(swapFiles: Set[File]): Unit
----

`completeSwapOperations`...FIXME

NOTE: `completeSwapOperations` is used exclusively when `Log` is requested to <<loadSegments, loadSegments>>.

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| nextOffsetMetadata
| [[nextOffsetMetadata]][[logEndOffsetMetadata]] `LogOffsetMetadata` (_log end offset_) of the next message that will be <<append, appended>> to the log

* Initialized right when `Log` is <<creating-instance, created>>

* Updated when <<updateLogEndOffset, updateLogEndOffset>>

Used when:

* `Log` is <<creating-instance, created>> and then requested to <<append, append>>, <<read, read>>, <<roll, roll>>, and for the <<logEndOffset, logEndOffset>>

* `Replica` is requested for <<kafka-cluster-Replica.adoc#logEndOffsetMetadata, logEndOffsetMetadata>>

| segments
| [[segments]] https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/ConcurrentSkipListMap.html[java.util.concurrent.ConcurrentSkipListMap] of `Longs` and their <<kafka-log-LogSegment.adoc#, LogSegments>>

* Cleared in <<loadSegments, loadSegments>> just before <<loadSegmentFiles, loadSegmentFiles>>

Used when...FIXME

|===
