== [[Log]] Partition Log

`Log` is a *partition log* that is <<apply, created>> when `LogManager` is requested to <<kafka-log-LogManager.adoc#loadLog, loadLog>> (when `LogManager` is <<kafka-log-LogManager.adoc#creating-instance, created>>) and <<kafka-log-LogManager.adoc#getOrCreateLog, getOrCreateLog>>.

[[CleanShutdownFile]]
`Log` uses *.kafka_cleanshutdown* for...FIXME

[[DeleteDirSuffix]]
`Log` uses *-delete* suffix for...FIXME

[[isFuture]]
`Log` is *isFuture* when...FIXME

`Log` uses a <<scheduler, Scheduler>> to schedule the <<background-tasks, background tasks>>.

[[background-tasks]]
.Log's Background Tasks
[cols="1m,1,1,2",options="header",width="100%"]
|===
| Name
| Period
| Delay
| Description

| PeriodicProducerExpirationCheck
| <<producerIdExpirationCheckIntervalMs, producerIdExpirationCheckIntervalMs>>
| <<producerIdExpirationCheckIntervalMs, producerIdExpirationCheckIntervalMs>>
| [[PeriodicProducerExpirationCheck]] Requests the <<producerStateManager, ProducerStateManager>> to <<kafka-log-ProducerStateManager.adoc#removeExpiredProducers, removeExpiredProducers>>

Scheduled immediately when `Log` is <<creating-instance, created>>.

| flush-log
| `-1` (once)
| `0L`
| [[flush-log]] <<flush, flush>>

Scheduled when `Log` is requested to <<roll, roll>>.

| delete-file
| `-1` (once)
| <<kafka-log-LogConfig.adoc#fileDeleteDelayMs, file.delete.delay.ms>>
| [[delete-file]] <<deleteSeg, deleteSeg>>

Scheduled when `Log` is requested to <<asyncDeleteSegment, asyncDeleteSegment>>.

|===

[[logIdent]]
`Log` uses *[Log partition=[topicPartition], dir=[parent]]* as the logging prefix (aka `logIdent`).

[[logging]]
[TIP]
====
Enable `ALL` logging levels for `kafka.log.Log` logger to see what happens inside.

Add the following line to `log4j.properties`:

```
log4j.logger.kafka.log.Log=ALL
```

Refer to <<kafka-logging.adoc#, Logging>>.
====

=== [[KafkaMetricsGroup]][[metrics]] Performance Metrics

`Log` is a <<kafka-metrics-KafkaMetricsGroup.adoc#, KafkaMetricsGroup>> with the following performance metrics.

.Log's Performance Metrics
[cols="30m,70",options="header",width="100%"]
|===
| Metric Name
| Description

| NumLogSegments
| [[NumLogSegments]]

| LogEndOffset
| [[LogEndOffset-metrics]]

| LogStartOffset
| [[LogStartOffset]]

| Size
| [[Size]]

|===

The performance metrics are registered in *kafka.log:type=Log* group.

.Log in jconsole
image::images/Log-jconsole.png[align="center"]

=== [[creating-instance]] Creating Log Instance

`Log` takes the following to be created:

* [[dir]] Log directory
* [[config]] <<kafka-log-LogConfig.adoc#, LogConfig>>
* [[logStartOffset]] `logStartOffset`
* [[recoveryPoint]] `recoveryPoint`
* [[scheduler]] <<kafka-Scheduler.adoc#, Scheduler>>
* [[brokerTopicStats]] <<kafka-server-BrokerTopicStats.adoc#, BrokerTopicStats>>
* [[time]] `Time`
* [[maxProducerIdExpirationMs]] `maxProducerIdExpirationMs`
* [[producerIdExpirationCheckIntervalMs]] `producerIdExpirationCheckIntervalMs`
* [[topicPartition]] `TopicPartition`
* [[producerStateManager]] `ProducerStateManager`
* [[logDirFailureChannel]] `LogDirFailureChannel`

`Log` initializes the <<internal-properties, internal properties>>.

[[creating-instance-nextOffsetMetadata]]
While being created, `Log`...FIXME

=== [[apply]] Creating Log Instance -- `apply` Factory Method

[source, scala]
----
apply(
  dir: File,
  config: LogConfig,
  logStartOffset: Long,
  recoveryPoint: Long,
  scheduler: Scheduler,
  brokerTopicStats: BrokerTopicStats,
  time: Time = Time.SYSTEM,
  maxProducerIdExpirationMs: Int,
  producerIdExpirationCheckIntervalMs: Int,
  logDirFailureChannel: LogDirFailureChannel): Log
----

`apply`...FIXME

NOTE: `apply` is used when `LogManager` is requested to <<kafka-log-LogManager.adoc#loadLog, loadLog>> and <<kafka-log-LogManager.adoc#getOrCreateLog, getOrCreateLog>>.

=== [[roll]] `roll` Method

[source, scala]
----
roll(
  expectedNextOffset: Option[Long] = None): LogSegment
----

`roll`...FIXME

NOTE: `roll` is used when `Log` is requested to <<deleteSegments, deleteSegments>> and <<maybeRoll, maybeRoll>>.

=== [[maybeRoll]] `maybeRoll` Internal Method

[source, scala]
----
maybeRoll(
  messagesSize: Int,
  appendInfo: LogAppendInfo): LogSegment
----

`maybeRoll`...FIXME

NOTE: `maybeRoll` is used exclusively when `Log` is requested to <<append, append>>.

=== [[asyncDeleteSegment]] `asyncDeleteSegment` Internal Method

[source, scala]
----
asyncDeleteSegment(segment: LogSegment): Unit
----

`asyncDeleteSegment`...FIXME

NOTE: `asyncDeleteSegment` is used when `Log` is requested to <<deleteSegment, deleteSegment>> and <<replaceSegments, replaceSegments>>.

=== [[flush]] `flush` Method

[source, scala]
----
flush(offset: Long): Unit
----

`flush`...FIXME

NOTE: `flush` is used when...FIXME

=== [[deleteSeg]] `deleteSeg` Internal Method

[source, scala]
----
deleteSeg(): Unit
----

`deleteSeg`...FIXME

NOTE: `deleteSeg` is used when...FIXME

=== [[appendAsLeader]] `appendAsLeader` Method

[source, scala]
----
appendAsLeader(
  records: MemoryRecords,
  leaderEpoch: Int,
  isFromClient: Boolean = true): LogAppendInfo
----

`appendAsLeader` simply <<append, append>> with the `assignOffsets` flag on.

NOTE: `appendAsLeader` is used exclusively when `Partition` is requested to <<kafka-cluster-Partition.adoc#appendRecordsToLeader, appendRecordsToLeader>>.

=== [[appendAsFollower]] `appendAsFollower` Method

[source, scala]
----
appendAsFollower(records: MemoryRecords): LogAppendInfo
----

`appendAsFollower` simply <<append, append>> (with the `isFromClient` and `assignOffsets` flags off, and the `leaderEpoch` being `-1`).

NOTE: `appendAsFollower` is used exclusively when `Partition` is requested to <<kafka-cluster-Partition.adoc#doAppendRecordsToFollowerOrFutureReplica, doAppendRecordsToFollowerOrFutureReplica>>.

=== [[append]] `append` Internal Method

[source, scala]
----
append(
  records: MemoryRecords,
  isFromClient: Boolean,
  interBrokerProtocolVersion: ApiVersion,
  assignOffsets: Boolean,
  leaderEpoch: Int): LogAppendInfo
----

`append`...FIXME

NOTE: `append` is used when `Log` is requested to <<appendAsLeader, appendAsLeader>> (with `assignOffsets` flag on) and <<appendAsFollower, appendAsFollower>> (with `assignOffsets` and `isFromClient` flags off).

==== [[analyzeAndValidateRecords]] `analyzeAndValidateRecords` Internal Method

[source, scala]
----
analyzeAndValidateRecords(
  records: MemoryRecords,
  isFromClient: Boolean): LogAppendInfo
----

`analyzeAndValidateRecords`...FIXME

NOTE: `analyzeAndValidateRecords` is used exclusively when `Log` is requested to <<append, append>>.

=== [[deleteSegment]] `deleteSegment` Internal Method

[source, scala]
----
deleteSegment(segment: LogSegment): Unit
----

`deleteSegment`...FIXME

NOTE: `deleteSegment` is used when `Log` is requested to <<recoverLog, recoverLog>>, <<deleteSegments, deleteSegments>>, <<roll, roll>>, <<truncateTo, truncateTo>>, and <<truncateFullyAndStartAt, truncateFullyAndStartAt>>.

=== [[replaceSegments]] `replaceSegments` Internal Method

[source, scala]
----
replaceSegments(
  newSegments: Seq[LogSegment],
  oldSegments: Seq[LogSegment],
  isRecoveredSwapFile: Boolean = false): Unit
----

`replaceSegments`...FIXME

[NOTE]
====
`replaceSegments` is used when:

* `Log` is requested to <<completeSwapOperations, completeSwapOperations>> and <<splitOverflowedSegment, splitOverflowedSegment>>

* `Cleaner` is requested to `cleanSegments`
====

=== [[recoverLog]] `recoverLog` Internal Method

[source, scala]
----
recoverLog(): Long
----

`recoverLog`...FIXME

NOTE: `recoverLog` is used exclusively when `Log` is requested to <<loadSegments, loadSegments>>.

=== [[deleteSegments]] `deleteSegments` Internal Method

[source, scala]
----
deleteSegments(deletable: Iterable[LogSegment]): Int
----

`deleteSegments`...FIXME

NOTE: `deleteSegments` is used exclusively when `Log` is requested to <<deleteOldSegments, deleteOldSegments>>.

=== [[truncateTo]] `truncateTo` Internal Method

[source, scala]
----
truncateTo(targetOffset: Long): Boolean
----

`truncateTo`...FIXME

NOTE: `truncateTo` is used exclusively when `LogManager` is requested to <<kafka-log-LogManager.adoc#truncateTo, truncateTo>>.

=== [[truncateFullyAndStartAt]] `truncateFullyAndStartAt` Internal Method

[source, scala]
----
truncateFullyAndStartAt(newOffset: Long): Unit
----

`truncateFullyAndStartAt`...FIXME

[NOTE]
====
`truncateFullyAndStartAt` is used when:

* `Log` is requested to <<truncateTo, truncateTo>>

* `LogManager` is requested to <<kafka-log-LogManager.adoc#truncateFullyAndStartAt, truncateFullyAndStartAt>>
====

=== [[deleteOldSegments]] `deleteOldSegments` Method

[source, scala]
----
deleteOldSegments(): Long
// Private API
deleteOldSegments(
  predicate: (LogSegment, Option[LogSegment]) => Boolean,
  reason: String): Int
----

`deleteOldSegments`...FIXME

NOTE: `deleteOldSegments` is used when...FIXME

=== [[completeSwapOperations]] `completeSwapOperations` Internal Method

[source, scala]
----
completeSwapOperations(swapFiles: Set[File]): Unit
----

`completeSwapOperations`...FIXME

NOTE: `completeSwapOperations` is used when...FIXME

=== [[splitOverflowedSegment]] `splitOverflowedSegment` Internal Method

[source, scala]
----
splitOverflowedSegment(segment: LogSegment): List[LogSegment]
----

`splitOverflowedSegment`...FIXME

NOTE: `splitOverflowedSegment` is used when...FIXME

=== [[loadProducerState]] `loadProducerState` Internal Method

[source, scala]
----
loadProducerState(lastOffset: Long, reloadFromCleanShutdown: Boolean): Unit
----

`loadProducerState`...FIXME

NOTE: `loadProducerState` is used when `Log` is <<creating-instance, created>> and requested to <<truncateTo, truncateTo>>.

=== [[onHighWatermarkIncremented]] `onHighWatermarkIncremented` Method

[source, scala]
----
onHighWatermarkIncremented(highWatermark: Long): Unit
----

`onHighWatermarkIncremented`...FIXME

NOTE: `onHighWatermarkIncremented` is used when...FIXME

=== [[parseTopicPartitionName]] `parseTopicPartitionName` Object Method

[source, scala]
----
parseTopicPartitionName(dir: File): TopicPartition
----

`parseTopicPartitionName`...FIXME

NOTE: `parseTopicPartitionName` is used when...FIXME

=== [[offsetFromFileName]] `offsetFromFileName` Object Method

[source, scala]
----
offsetFromFileName(filename: String): Long
----

`offsetFromFileName`...FIXME

NOTE: `offsetFromFileName` is used when `Log` is requested to <<removeTempFilesAndCollectSwapFiles, removeTempFilesAndCollectSwapFiles>> (right when <<creating-instance, created>>) and <<offsetFromFile, offsetFromFile>>.

=== [[offsetFromFile]] `offsetFromFile` Object Method

[source, scala]
----
offsetFromFile(file: File): Long
----

`offsetFromFile`...FIXME

NOTE: `offsetFromFile` is used when...FIXME

=== [[read]] `read` Method

[source, scala]
----
read(
  startOffset: Long,
  maxLength: Int,
  maxOffset: Option[Long],
  minOneMessage: Boolean,
  includeAbortedTxns: Boolean): FetchDataInfo
----

`read`...FIXME

NOTE: `read` is used when...FIXME

=== [[updateLogEndOffset]] `updateLogEndOffset` Internal Method

[source, scala]
----
updateLogEndOffset(messageOffset: Long): Unit
----

`updateLogEndOffset`...FIXME

NOTE: `updateLogEndOffset` is used when...FIXME

=== [[logEndOffset]] `logEndOffset` Method

[source, scala]
----
logEndOffset: Long
----

`logEndOffset`...FIXME

NOTE: `logEndOffset` is used when...FIXME

=== [[loadSegments]] `loadSegments` Internal Method

[source, scala]
----
loadSegments(): Long
----

`loadSegments`...FIXME

NOTE: `loadSegments` is used exclusively when `Log` is <<creating-instance, created>> (to create a <<nextOffsetMetadata, LogOffsetMetadata>>).

=== [[removeTempFilesAndCollectSwapFiles]] `removeTempFilesAndCollectSwapFiles` Internal Method

[source, scala]
----
removeTempFilesAndCollectSwapFiles(): Set[File]
----

`removeTempFilesAndCollectSwapFiles`...FIXME

NOTE: `removeTempFilesAndCollectSwapFiles` is used exclusively when `Log` is requested to <<loadSegments, loadSegments>> (right when <<creating-instance, created>>).

=== [[loadSegmentFiles]] `loadSegmentFiles` Internal Method

[source, scala]
----
loadSegmentFiles(): Unit
----

`loadSegmentFiles`...FIXME

NOTE: `loadSegmentFiles` is used exclusively when `Log` is requested to <<loadSegments, loadSegments>> (right when <<creating-instance, created>>).

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| nextOffsetMetadata
| [[nextOffsetMetadata]][[logEndOffsetMetadata]] `LogOffsetMetadata` (_log end offset_) of the next message that will be <<append, appended>> to the log

* Initialized right when `Log` is <<creating-instance, created>>

* Updated when <<updateLogEndOffset, updateLogEndOffset>>

Used when:

* `Log` is <<creating-instance, created>> and then requested to <<append, append>>, <<read, read>>, <<roll, roll>>, and for the <<logEndOffset, logEndOffset>>

* `Replica` is requested for <<kafka-cluster-Replica.adoc#logEndOffsetMetadata, logEndOffsetMetadata>>

| segments
| [[segments]] https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/ConcurrentSkipListMap.html[java.util.concurrent.ConcurrentSkipListMap] of `Longs` and their <<kafka-log-LogSegment.adoc#, LogSegments>>

Used when...FIXME

|===
