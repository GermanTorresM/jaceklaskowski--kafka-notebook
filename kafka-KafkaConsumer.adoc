== [[KafkaConsumer]] KafkaConsumer -- Main Class For Kafka Consumers

`KafkaConsumer` is the main public class that you and other Kafka developers use to write link:kafka-consumers.adoc[Kafka consumers] that consume records from a Kafka cluster.

.KafkaConsumer
image::images/KafkaConsumer.png[align="center"]

`KafkaConsumer` is a part of the public API and is <<creating-instance, created>> with properties and (key and value) deserializers as configuration.

NOTE: link:kafka-properties.adoc#bootstrap.servers[bootstrap.servers] and link:kafka-properties.adoc#group.id[group.id] properties are mandatory. They usually appear in the code as `ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG` and  `ConsumerConfig.GROUP_ID_CONFIG` values, respectively.

`KafkaConsumer` follows the link:kafka-Consumer.adoc[Consumer contract].

[source, scala]
----
// sandbox/kafka-sandbox
val bootstrapServers = "localhost:9092"
val groupId = "kafka-sandbox"
import org.apache.kafka.clients.consumer.ConsumerConfig
val configs: Map[String, Object] = Map(
  // required properties
  ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> bootstrapServers,
  ConsumerConfig.GROUP_ID_CONFIG -> groupId
)
import org.apache.kafka.common.serialization.StringDeserializer
val keyDeserializer = new StringDeserializer
val valueDeserializer = new StringDeserializer

import scala.collection.JavaConverters._
import org.apache.kafka.clients.consumer.KafkaConsumer
val consumer = new KafkaConsumer[String, String](
  configs.asJava,
  keyDeserializer,
  valueDeserializer)
----

`KafkaConsumer` registers itself in JMX with *kafka.consumer* prefix.

CAUTION: FIXME How does the JMX registration happen?

[IMPORTANT]
====
`KafkaConsumer` does not support multi-threaded access. You should use only one thread per `KafkaConsumer` instance.

`KafkaConsumer` uses light locks protecting itself from multi-threaded access and reports `ConcurrentModificationException` when it happens.

```
KafkaConsumer is not safe for multi-threaded access
```
====

[[internal-registries]]
.KafkaConsumer's Internal Properties (e.g. Registries and Counters)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[client]] `client`
| link:kafka-ConsumerNetworkClient.adoc[ConsumerNetworkClient]

Used mainly _(?)_ to create the <<fetcher, Fetcher>> and <<coordinator, ConsumerCoordinator>>

Used also in <<poll, poll>>, <<pollOnce, pollOnce>> and <<wakeup, wakeup>> (but _I_ think the usage should be limited to create <<fetcher, Fetcher>> and <<coordinator, ConsumerCoordinator>>)

| [[clientId]] `clientId`
|

| [[coordinator]] `coordinator`
| link:kafka-ConsumerCoordinator.adoc[ConsumerCoordinator]

| [[fetcher]] `fetcher`
| link:kafka-Fetcher.adoc[Fetcher]

Created right when `KafkaConsumer` is <<creating-instance, created>>.

Used when...FIXME

| [[interceptors]] `interceptors`
| `ConsumerInterceptors` that holds link:kafka-ConsumerInterceptor.adoc[ConsumerInterceptor] instances (defined using link:kafka-properties.adoc#interceptor.classes[interceptor.classes] setting).

Used when...FIXME

| [[metadata]] `metadata`
| link:kafka-Metadata.adoc[Metadata]

Created right when `KafkaConsumer` is <<creating-instance, created>>.

Used when...FIXME

| [[metrics]] `metrics`
| `Metrics`

| [[retryBackoffMs]] `retryBackoffMs`
| link:kafka-properties-retry-backoff-ms.adoc[retry.backoff.ms] property or a user-defined value

| [[requestTimeoutMs]] `requestTimeoutMs`
a| Corresponds to link:kafka-properties.adoc#request.timeout.ms[request.timeout.ms] property

`KafkaConsumer` reports `ConfigException` when smaller or equal than link:kafka-properties.adoc#session_timeout_ms[session.timeout.ms] and link:kafka-properties.adoc#fetch_max_wait_ms[fetch.max.wait.ms] properties.

| [[subscriptions]] `subscriptions`
| `SubscriptionState` for link:kafka-properties.adoc#auto.offset.reset[auto.offset.reset] setting.

Created when `KafkaConsumer` <<creating-instance, is created>>.
|===

[[logging]]
[TIP]
====
Enable `DEBUG` or `TRACE` logging levels for `org.apache.kafka.clients.consumer.KafkaConsumer` logger to see what happens inside.

Add the following line to `config/tools-log4j.properties`:

```
log4j.logger.org.apache.kafka.clients.consumer.KafkaConsumer=TRACE
```

Refer to link:kafka-logging.adoc[Logging].
====

=== [[configureClusterResourceListeners]] `configureClusterResourceListeners` Internal Method

[source, java]
----
ClusterResourceListeners configureClusterResourceListeners(
  Deserializer<K> keyDeserializer,
  Deserializer<V> valueDeserializer,
  List<?>... candidateLists)
----

`configureClusterResourceListeners`...FIXME

NOTE: `configureClusterResourceListeners` is used when...FIXME

=== [[unsubscribe]] `unsubscribe` Method

CAUTION: FIXME

=== [[subscribe-pattern]] Subscribing to Topics Matching Pattern -- `subscribe` Method

[source, scala]
----
void subscribe(Collection<String> topics)
----

NOTE: `subscribe` is a part of link:kafka-Consumer.adoc#contract[Consumer Contract].

`subscribe`...FIXME

NOTE: `subscribe` is used when...FIXME

=== [[subscribe]] Subscribing to Topics -- `subscribe` Method

[source, java]
----
void subscribe(Collection<String> topics) // <1>
void subscribe(Collection<String> topics, ConsumerRebalanceListener listener)
----
<1> A short-hand for the other subscribe with `NoOpConsumerRebalanceListener` as `ConsumerRebalanceListener`

`subscribe` subscribes `KafkaConsumer` to the given topics.

[source, scala]
----
val topics = Seq("topic1")
println(s"Subscribing to ${topics.mkString(", ")}")

import scala.collection.JavaConverters._
consumer.subscribe(topics.asJava)
----

Internally, `subscribe` prints out the following DEBUG message to the logs:

```
DEBUG Subscribed to topic(s): [comma-separated topics]
```

`subscribe` then requests <<subscriptions, SubscriptionState>> to `subscribe` for the `topics` and `listener`.

In the end, `subscribe` requests <<subscriptions, SubscriptionState>> for `groupSubscription` that it then passes along to <<metadata, Metadata>> to link:kafka-Metadata.adoc#setTopics[set the topics to track].

.KafkaConsumer subscribes to topics
image::images/KafkaConsumer-subscribe.png[align="center"]

=== [[poll]] Poll Specified Milliseconds For ConsumerRecords per TopicPartitions -- `poll` Method

[source, java]
----
ConsumerRecords<K, V> poll(long timeout)
----

`poll` polls for new records until `timeout` expires.

NOTE: `KafkaConsumer` has to be subscribed to some topics or assigned partitions before calling <<poll, poll>>.

NOTE: The input `timeout` should be `0` or greater and represents the milliseconds to poll for records.

[source, scala]
----
val seconds = 10
while (true) {
  println(s"Polling for records for $seconds secs")
  val records = consumer.poll(seconds * 1000)
  // do something with the records here
}
----

Internally, `poll` starts by <<pollOnce, polling once>> (for `timeout` milliseconds).

If there are records available, `poll` checks <<fetcher, Fetcher>> for link:kafka-Fetcher.adoc#sendFetches[sendFetches] and <<client, ConsumerNetworkClient>> for link:kafka-ConsumerNetworkClient.adoc#pendingRequestCount[pendingRequestCount] flag. If either is positive, `poll` requests <<client, ConsumerNetworkClient>> to link:kafka-ConsumerNetworkClient.adoc#pollNoWakeup[pollNoWakeup].

CAUTION: FIXME Make the above more user-friendly

`poll` returns the available `ConsumerRecords` directly when no <<interceptors, ConsumerInterceptors>> are defined or passes them through <<interceptors, ConsumerInterceptors>> using link:kafka-ConsumerInterceptor.adoc#onConsume[onConsume].

CAUTION: FIXME Make the above more user-friendly, e.g. when could `interceptors` be empty?

.KafkaConsumer polls topics
image::images/KafkaConsumer-poll.png[align="center"]

NOTE: `poll` is a part of link:kafka-consumers.adoc#poll[Consumer contract] to...FIXME

=== [[partitionsFor]] Getting Partitions For Topic -- `partitionsFor` Method

CAUTION: FIXME

=== [[endOffsets]] `endOffsets` Method

CAUTION: FIXME

=== [[offsetsForTimes]] `offsetsForTimes` Method

CAUTION: FIXME

=== [[updateFetchPositions]] `updateFetchPositions` Method

CAUTION: FIXME

=== [[pollOnce]] Polling Once for ConsumerRecords per TopicPartition -- `pollOnce` Internal Method

CAUTION: FIXME

=== [[listTopics]] Requesting Metadata for All Topics (From Brokers) -- `listTopics` Method

[source, java]
----
Map<String, List<PartitionInfo>> listTopics()
----

Internally, `listTopics` simply requests <<fetcher, Fetcher>> for link:kafka-Fetcher.adoc#getAllTopicMetadata[metadata for all topics] and returns it.

[source, scala]
----
consumer.listTopics().asScala.foreach { case (name, partitions) =>
  println(s"topic: $name (partitions: ${partitions.size()})")
}
----

NOTE: `listTopics` uses <<requestTimeoutMs, requestTimeoutMs>> that corresponds to link:kafka-properties.adoc#request.timeout.ms[request.timeout.ms] property.

=== [[beginningOffsets]] `beginningOffsets` Method

[source, java]
----
Map<TopicPartition, Long> beginningOffsets(Collection<TopicPartition> partitions)
----

`beginningOffsets` requests <<fetcher, Fetcher>> for link:kafka-Fetcher.adoc#beginningOffsets[beginningOffsets] and returns it.

=== [[creating-instance]] Creating KafkaConsumer Instance

`KafkaConsumer` takes the following when created:

* [[configs]] Consumer configuration (that is converted internally to link:kafka-ConsumerConfig.adoc[ConsumerConfig])
* [[keyDeserializer]] link:kafka-Deserializer.adoc[Deserializer] for keys
* [[valueDeserializer]] link:kafka-Deserializer.adoc[Deserializer] for values

`KafkaConsumer` initializes the <<internal-registries, internal registries and counters>>.

NOTE: `KafkaConsumer` API offers other constructors that in the end use the <<creating-instance-public, public 3-argument constructor>> that in turn passes the call on to the <<creating-instance-internal, private internal constructor>>.

==== [[creating-instance-public]] KafkaConsumer Public Constructor

[source, java]
----
// Public API
KafkaConsumer(
  Map<String, Object> configs,
  Deserializer<K> keyDeserializer,
  Deserializer<V> valueDeserializer)
----

When created, `KafkaConsumer` adds the <<keyDeserializer, keyDeserializer>> and <<valueDeserializer, valueDeserializer>> to <<configs, configs>> (as link:kafka-properties.adoc#key.deserializer[key.deserializer] and link:kafka-properties.adoc#value.deserializer[value.deserializer] properties respectively) and creates a link:kafka-ConsumerConfig.adoc[ConsumerConfig].

`KafkaConsumer` passes the call on to the <<creating-instance-internal, internal constructor>>.

==== [[creating-instance-internal]] KafkaConsumer Internal Constructor

[source, java]
----
KafkaConsumer(
  ConsumerConfig config,
  Deserializer<K> keyDeserializer,
  Deserializer<V> valueDeserializer)
----

When called, the internal `KafkaConsumer` constructor prints out the following DEBUG message to the logs:

```
DEBUG Starting the Kafka consumer
```

`KafkaConsumer` sets the internal <<requestTimeoutMs, requestTimeoutMs>> to <<request_timeout_ms, request.timeout.ms>> property.

`KafkaConsumer` sets the internal <<clientId, clientId>> to link:kafka-properties.adoc#client.id[client.id] or generates one with prefix *consumer-* (starting from 1) if not set.

`KafkaConsumer` sets the internal <<metrics, Metrics>> (and `JmxReporter` with *kafka.consumer* prefix).

`KafkaConsumer` sets the internal <<retryBackoffMs, retryBackoffMs>> to link:kafka-properties.adoc#retry.backoff.ms[retry.backoff.ms] property.

CAUTION: FIXME Finish me!

`KafkaConsumer` creates the internal <<metadata, Metadata>> with the following arguments:

1. <<retryBackoffMs, retryBackoffMs>>
1. link:kafka-properties.adoc#metadata.max.age.ms[metadata.max.age.ms]
1. `allowAutoTopicCreation` enabled
1. `topicExpiryEnabled` disabled
1. link:kafka-ClusterResourceListener.adoc[ClusterResourceListeners] with user-defined list of link:kafka-ConsumerInterceptor.adoc[ConsumerInterceptors] in link:kafka-properties.adoc#interceptor.classes[interceptor.classes] property

`KafkaConsumer` link:kafka-Metadata.adoc#update[updates] `metadata` with link:kafka-properties.adoc#bootstrap.servers[bootstrap.servers].

CAUTION: FIXME Finish me!

`KafkaConsumer` creates a link:kafka-NetworkClient.adoc[NetworkClient] with...FIXME

CAUTION: FIXME Finish me!

`KafkaConsumer` creates <<fetcher, Fetcher>> with the following properties:

* link:kafka-properties.adoc#fetch.min.bytes[fetch.min.bytes]
* link:kafka-properties.adoc#fetch.max.bytes[fetch.max.bytes]
* link:kafka-properties.adoc#fetch.max.wait.ms[fetch.max.wait.ms]
* link:kafka-properties.adoc#max.partition.fetch.bytes[max.partition.fetch.bytes]
* link:kafka-properties.adoc#max.poll.records[max.poll.records]
* link:kafka-properties.adoc#check.crcs[check.crcs]

In the end, `KafkaConsumer` prints out the following DEBUG message to the logs:

```
DEBUG Kafka consumer created
```

Any issues while creating a `KafkaConsumer` are reported as `KafkaException`.

```
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
```

=== [[wakeup]] `wakeup` Method

[source, scala]
----
void wakeup()
----

NOTE: `wakeup` is a part of link:kafka-Consumer.adoc#wakeup[Consumer Contract].

`wakeup` simply requests <<client, ConsumerNetworkClient>> to link:kafka-ConsumerNetworkClient.adoc#wakeup[wakeup].

.KafkaConsumer's wakeup Method
image::images/KafkaConsumer-wakeup.png[align="center"]

[NOTE]
====
Quoting `wakeup` of Java's link:++http://download.java.net/java/jdk9/docs/api/java/nio/channels/Selector.html#wakeup--++[java.nio.channels.Selector] given `wakeup` simply passes through the intermediaries and in the end triggers it.

> Causes the first selection operation that has not yet returned to return immediately.

Read about Selection in http://download.java.net/java/jdk9/docs/api/java/nio/channels/Selector.html#selop[java.nio.channels.Selector]'s javadoc.
====

NOTE: `wakeup` is used when...FIXME
